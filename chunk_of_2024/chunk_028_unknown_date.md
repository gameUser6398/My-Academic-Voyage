

1 背景

我们在两个同时进行的任务中对~106M种蛋白质（代表整个已知的蛋白质空间）进行了ProteinBert预训练。

第一个任务是蛋白质序列的双向语言建模。

第二项任务是基因本体论（GO）注释预测，它捕捉不同的蛋白质功能（Ashburner等人，2000）。GO注释是在整

个蛋白质水平上定义的一组人工策划的~45K术语，涵盖了所有生物体的整个蛋白质空间。它们涵盖了分子功能、

生物过程和亚细胞位置。

与经典的Transformers不同，ProteinBERT分离了局部（字符级）和全局（全序列级）表示（以及输入和输出），

从而以原则的方式支持局部和全局任务的多任务处理。虽然ProteinBERT比现有模型小得多，速度也快得多，但它

在一组不同的基准上接近或超过了最先进的性能。

代码和预训练模型权重可在 https://github.com/nadavbra/protein_bert 获得。

2 材料和方法

2.1 数据

2.1.1 用于预训练的蛋白质数据集

ProteinBERT在源自UniProtKB / UniRef106的∼90M蛋白质上进行预训练。UniRef90提供了一组非冗余的蛋白

质簇，共享至少90%的序列同一性。每个簇由单个代表性蛋白质表示，确保蛋白质空间的相对均匀覆盖。对于每

种蛋白质，我们提取了其氨基酸序列和相关的GO注释（根据UniProtKB）。我们只考虑了 UniRef8943 中至少

出现 100 次的 90 个最常见的 GO 注释。在 ∼106M UniRef90 蛋白中，∼46M 至少具有 8943 个注释中的一个

（每个蛋白质平均有 2.3 个注释，跨 ∼46M 蛋白质）。

2.1.2 蛋白质评估

