分区 文献阅读 的第 67 页

2.1.2 蛋白质评估

分区 文献阅读 的第 68 页

九个基准中的四个（二级结构、远程同源性、荧光和稳定性）取自 TAPE（评估蛋白质嵌入的任

务），这是一套用于评估蛋白质序列模型的标准化基准。此外，我们还引入了五个新的基

准。ProteinBERT与其他模型之间的比较是在四个TAPE基准上进行的。还对五项新基准进行了内部

评价（见第3节）。

2.2 序列和注释编码

蛋白质序列被编码为整数标记序列。我们使用了代表26种标准氨基酸的20个独特标记，硒代半胱氨

酸（U），一个未定义的氨基酸（X），另一个氨基酸（OTHER）和3个额外的标记（START，

END和PAD）。对于每个序列，分别在第一个氨基酸之前和最后一个氨基酸之后添加 START

和 END 标记。PAD 令牌被添加到短于为小批量选择的序列长度的 pad 序列中。

ProteinBERT的架构（与大多数深度学习模型一样）规定每个小批量具有固定的序列长度。我们包

括了START和END标记，以帮助模型解释比所选序列长度更长的蛋白质。当编码的蛋白质长度超过

所选序列长度时，我们选择了蛋白质的随机子序列，省略了其两端中的至少一个。缺少 START 或

END 标记允许模型识别它只接收序列的一部分。

每个序列的GO注释被编码为固定大小的二进制向量（8943），其中所有条目均为零，除了与蛋白

质相关的GO注释对应的条目。当没有向模型提供有关 GO 注释的信息时（例如，在对基准进行微

调和评估期间），向量将设置为全零。

2.3 蛋白质序列和注释的自监督预训练

为了学习蛋白质表征，ProteinBERT对从UniRef90中提取的蛋白质序列和GO注释进行了预训练。

该模型接收到损坏的输入（蛋白质序列和GO注释），并且必须恢复未损坏的数据。蛋白质序列的

分区 文献阅读 的第 69 页

该模型接收到损坏的输入（蛋白质序列和GO注释），并且必须恢复未损坏的数据。蛋白质序列的

破坏是通过以5%的概率随机替换令牌（即以95%的概率保留原始令牌，或用具有5%概率的统一选

择的随机令牌替换它）来完成的。通过以25%的概率随机删除现有注释，并为每个与蛋白质无关的

注释添加概率为0.01%的随机错误注释来破坏输入GO注释。对于50%的加工蛋白质，我们完全删除

了所有输入注释（即给出一个全零的输入向量），以强制模型仅从序列中预测GO注释（就像所有

测试基准一样）。总之，所描述的预训练是一项双重任务，其中模型必须恢复蛋白质序列及其已知

的GO注释。后一项任务与蛋白质研究的许多领域相关，因为GO术语涵盖了广泛的功能。

为了避免测试基准中蛋白质的GO注释中的信息泄漏，我们删除了与基准测试集中任何记录序列相

似性至少为40%的蛋白质的GO注释【使用具有默认参数的BLASTP】，从而确保ProteinBERT不会

获得有关测试集蛋白质的信息。

ProteinBERT的一个重要特征是序列长度的灵活性。为了避免将模型过度拟合到特定恒定长度的风

险，我们定期（每 15 分钟的训练）使用 128、512 或 1024 个标记的长度切换蛋白质序列的编码

长度。

单个GPU（Nvidia Quadro RTX 5000）的预训练速度为每秒280个蛋白质记录。我们在 28M 条记

录上训练了 670 天的模型（即在 6M 条记录的整个训练数据集上进行了 4 次迭代）。经过训练的

模型权重与我们的代码一起公开提供（见下文）。

2.4 对蛋白质基准的监督微调

在预训练之后，我们在一组不同的基准上微调和评估了模型。对于所有基准测试，ProteinBERT从

相同的预训练状态初始化，并通过相同的协议进行微调。最初，预训练模型的所有层都被冻结，只

有新添加的全连接层被允许训练多达 40 个 epoch。接下来，我们解冻所有层，并训练模型最多 40

个额外的 epoch。最后，我们为更大序列长度的最后一个时期训练了模型（参见补充方法).在所有

时期，我们降低了高原的学习率，并根据独立验证集应用了早期停止。然后在保留的测试集上进行

模型评估。在整个微调和基准评估过程中，没有利用有关GO注释的信息（即GO注释输入始终是一

个恒定的全零向量）。整个微调过程在单个GPU上花费了大约14分钟（在九个基准测试中平均）。

2.5 深度学习架构

虽然受到BERT的启发（Devlin等人，2018），但ProteinBERT的架构是不同的，包括一些创

新。ProteinBERT是一种去噪自动编码器（图1）。ProteinBERT的两个输入（和输出）是（i）蛋

白质序列（编码为氨基酸标记序列）和（ii）GO注释（编码为固定大小的二进制向量）。

分区 文献阅读 的第 70 页

ProteinBERT架构。ProteinBERT的架构灵感来自BERT。与标准变压器不同，ProteinBERT支持本

地（顺序）和全局数据。该模型由六个类似变压器的块组成，这些块操纵局部（左侧）和全局（右

侧）表示。每个这样的块通过完全连接和卷积层（在局部表示的情况下）操作这些表示，它们之间

具有跳过连接和规范化层。局部制图表达通过全局注意力层影响全局制图表达，全局制图表达通过

广播全连接层影响局部制图表达

模型架构由两条几乎并行的路(cid:1708)组成：一条用于局部表示，另一条用于全局表示（图 1）。局部表

示是形状的 3D 张量B×L×d当地� ×� ×� 当地哪里B� 是批量大小，L� 是小批量序列长度，并且

d当地� 当地是局部表示的通道数（我们使用d当地=128� 当地=128⁠

).全局表示是形状的 2D 张

量B×d全球� ×� 全球（使用d全球=512� 全球=512⁠

).在模型的第一层中，输入序列通过嵌入层

转换为局部表示的 3D 张量，该嵌入层具有d当地� 当地输出特征（独立应用且位置相同），输入

注释通过具有d全球� 全球输出功能。

局部和全局表示由一系列六个类似变压器的块处理，这些块在其隐藏层之间具有跳过连接和层归一

化。在每个块中，局部表示首先由一维卷积层转换，然后由（位置方面）全连接层转换。为了使每

个位置的局部表示基于短距离和远距离的其他位置，我们同时使用了窄（无膨胀）和宽（膨胀率为

1）卷积层。两种类型的卷积层的内核大小均为 5，步幅大小均为 9。因此，每个窄层的感受野为

1，每个宽层的感受野比前一层高9，这意味着第41块在输入序列上的感受野为6。另一方面，全局

表示由每个块两个简单的全连接层转换（它们之间有规范化）。模型的所有隐藏的全连接和卷积层

都使用GELU（高斯误差线性单元）激活（Hendrycks和Gimpel，241）。

分区 文献阅读 的第 71 页

都使用GELU（高斯误差线性单元）激活（Hendrycks和Gimpel，241）。

局部表示和全局表示之间的唯一信息流通过广播全连接层（从全局表示到局部表示）和全局注意力

层（从局部到全局表示）发生。广播层是完全连接的层，可转换d全球� 全球全球代表进入的特点d

当地� 当地局部制图表达的特征，然后在每个L� 序列位置。

受自我注意启发的全球注意力层（Vaswani et al.， 2017），具有线性（而不是二次）复杂性。自

我注意通过允许每个位置关注彼此的位置来获取一个输入序列并输出另一个序列，而全局注意力将

序列和全局固定大小向量作为输入，并输出通过根据全局输入向量关注每个局部输入位置而创建的

全局固定大小向量。形式上，单头全局注意力层将全局表示向量作为输入x∈Rd全球� ∈� � 全球

和局部表示向量L� 位置s1，...，sL∈Rd当地� 1,...,� � ∈� � 当地⁠ ，并输出全局输出y∈Rd价值

� ∈� � 价值⁠

.与自我注意类似，输出的计算公式为y=∑Li=1z我v我� =∑我=1� � 我� 我哪里v

我∈Rd价值� 我∈� � 价值是与每个位置关联的值i∈{1，...L}我∈{1,...� }和z我∈[0，1]� 我

∈[0,1]是分配给该职位的关注量（满足z1+⋯+zL=1� 1+⋯+� � =1⁠

).与自我注意一样，与每个位

置相关的值由下式计算v我=σ(Wvs我)� 我=σ� � � 我⁠ ，使用参数矩阵Wv∈Rd价值×d当地

� � ∈� � 价值×� 当地和激活功能σσ（我们选择了格鲁）。注意值的计算公式为z1，...，

zL=softmax{⟨q,k我⟩d.key√}Li=1� 1,...,� � =so英尺最大� ,� 我� .key我=1� ⁠ ，基于查询和键

向量q,k我∈Rd.key� ,� 我∈� � .key⁠

.请注意，虽然关键向量k1，...，kL� 1,...,� � 特定于每个

位置，查询向量q� 是全球性的。就像在自我注意中一样，关键向量由下式计算k我=谭(Wks我)�

我=谭� � � 我⁠ ，使用第二个参数矩阵Wk∈Rd.key×d当地� � ∈� � .key×� 当地⁠

.全局查询

向量的计算公式为q=谭(Wqx)� =谭� � � ⁠ ，使用第三个参数矩阵Wq∈Rd.key×d全球

� � ∈� � .key×� 全球⁠

.总体而言，单头全局注意力层在训练过程中使用三个参数矩阵拟合，

Wq� � ⁠

,Wk� � 和Wv� � ⁠

.它也由键维度参数化d.key� .key（我们使用d.key=64� .key=

64⁠

).通过应用得到多头全局注意力层n头� 头独立的单头全局注意力层（每个都有自己的参数）并

连接它们的输出，获得维度的输出n头⋅d价值� 头⋅� 价值（我们使用n头=4� 头=4跨所有 6 个块的

蛋白质BERT）。为了满足维度约束，ProteinBERT使用d价值=d全球n头=128� 价值=� 全球� 头

=128⁠

.

总体而言，ProteinBERT模型包括六个类似变压器的块，每个块中有四个全局注意力头。总的来

说，它包括约16M的可训练参数，使其比其他蛋白质语言模型小得多。相比之下，TAPE 变压器中

有 ∼38M 参数（Rao 等人，2019 年），BERT-base 中有 ∼110M（Devlin 等人，2018 年），

ProtTrans 的 ProtBert-BFD 中有 ∼430M（Elnaggar 等人，2021 年），ESM-650b 模型中有

1M 参数（Rives 等人，2021 年）和 ProtT3-XL-BFD 中的 5B（Elnaggar 等人，2021 年）。

ProteinBERT架构有几个吸引人的特性。最重要的是，整个架构与处理序列的长度无关，并且可以

应用于任何给定长度的序列，而无需更改其学习参数（我们的实验证明该模型确实可以(cid:1710)好地泛化

到不同的长度）。这种跨序列长度的良好通用性也是通过避免标准版本的BERT中使用的位置嵌入来

实现的，根据之前的报告（Neishi和Yoshinaga，2019）和我们的实验，并不总是(cid:1710)好地推广到比

训练数据中存在的序列长度更长的序列长度。相反，卷积层和每个序列开头和结尾使用的特殊标记

为模型提供了有关位置相对位置的信息。由于使用全局注意力而不是自我注意力，模型执行的计算

量仅随序列长度线性增长（与具有标准自我注意的模型中的二次增长相反）。这种线性增长也适用

于模型的内存消耗，允许ProteinBERT完整地处理极长的蛋白质序列（数万个氨基酸）。尽管进行

了这种简化，但由于局部和全局表示之间的交替信息流，局部表示和序列输出中的每个位置仍然可

以依赖于彼此位置的内容。最重要的是，宽卷积层和窄卷积层允许每个位置的表示依赖于大上下

文。通过依赖卷积层和注意力层，但避免循环层，网络执行的计算更有效，并且学习在序列长度方

分区 文献阅读 的第 72 页

文。通过依赖卷积层和注意力层，但避免循环层，网络执行的计算更有效，并且学习在序列长度方

面更稳定（因为没有长期依赖关系，并且网络执行的计算仅涉及固定数量的张量操作）

（Hochreiter et al.， 2001).值得注意的是，我们没有使用 dropout 或任何其他形式的正则化（除

了在微调模型时添加的最终全连接层，其中包括 dropout）。

在标记数据集上微调 ProteinBERT 时，将在其输出中添加另一个层。最后一层由模型的局部或全局

隐藏状态串联提供，具体取决于输出标签是局部还是全局。用于最后一层的激活取决于输出类型

（即分类标签的softmax激活，二进制标签的sigmoid激活或连续标签的无激活）。

2.6 可用性

用于ProteinBERT架构，预训练和微调的Python代码是开源的，可

在 https://github.com/nadavbra/protein_bert 获得。存储库还包括用于下载和生成数据集和基

准的预训练模型权重和代码。ProteinBERT在TensorFlow的Keras中实现（Abadi等人，2016;乔莱

特等人，2015 年）。

3 结果

3.1 预训练改善蛋白质建模

ProteinBERT在∼106M UniRef90记录上预训练了约6.4个epoch。我们看到，根据其他研究

（Rives 等人，2 年），即使在多个时期之后，语言建模损失在训练集上也会继续改善（即不饱

和）（图 2021）。另一方面，GO 注释任务确实显示饱和度。在预训练期间，我们会定期更改用于

编码输入和输出蛋白质序列（128、512 或 1024 个标记）的序列长度。我们观察到 128 令牌编码

的性能略低，但 512 和 1024 的性能相似。

无花果。2.

训练前损失。两个预训练任务的训练集损失：（i）蛋白质序列语言建模和（ii）GO注释恢复。在数
据集的前  28 批中，输入序列长度为 5 2、 024 或  00 个代币评估损失

3.2 ProteinBERT在各种蛋白质基准上取得了几乎最先进的结果
为了评估ProteinBE T， 们使用了九种基准，涵盖了蛋白质研究中的各种任务（参见表 中基准的
定义;所有基准的完整结果可在补充表S  .对于从TAPE中获取的四个基准（二级结构，远程同源性，
荧光和稳定性预测）， 们将 们的性(cid:14125)与其他最先进的序列模型进行了比较，这些模型已在具有
相同指标的相同基准上进行了评估（表2）。具体来说， 们与TAPE中评估的BE T变压器和LSTM模型
进行了比较（Alley等人，20 9年;贝普勒和伯杰，20 9年;Rao 等人，20 9 年）。 们还在二级结
构基准（这是四个 TAPE 基准中唯一一个已发表该模型结果的基准测试）上与
ProtT5（Elnaggar 等人，202  年）进行了比较。其他值得注意的蛋白质语言模型[例如ESM（Rives
等人，202 ）]没有直接可比较的已发表结果。值得注意的是，比较的来(cid:14362)TAPE的深度学习模型有大
约38M个参数，ProtT5-XL有3亿个参数，(cid:13884)ProteinBE T中的参数约为 6万个。 们评估了有和没有
预训练的ProteinBE T，观察到预训练对许多任务的表现有重大的积极影响。在这些基准测试中，
ProteinBE T显示出相当的性(cid:14125)，有时超过使用更多计算训练的类似，更大的模型。