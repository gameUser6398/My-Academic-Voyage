分区 文献阅读 的第 78 页

现有NLP+模型

2023年7月5日

15:11



【论文阅读】《BERT: Pre-training of Deep Bidirectional

Transformers for Language Understanding》 - 知乎 (zhihu.com)

ProteinBERT：蛋白质序列和功能的通用深度学习模型

来(cid:14362) <https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?login=true>

蛋白质的语言：NLP，机器学习和蛋白质序列 - PMC (nih.gov)

回顾了经典概念，如词袋，k-mers / n-gram和文本搜索，以及现代技术，如词嵌入，上下文

嵌入，深度学习和神经语言模型。特别是，我们专注于最近的创新，如掩蔽语言建模、自我监

督学习和基于注意力的模型

[1810.04805] BERT：用于语言理解的深度双向转换器的预训练 (arxiv.org)
BERT：用于语言理解的深度双向变压器的预训练

来(cid:14362) <https://arxiv.org/abs/1810.04805>

NLP  and function of protein

ProtTrans：通过自我监督学习理解生活语言 |IEEE 期刊和杂志 |IEEE Xplore

计算生物学和生物信息学提供了来自蛋白质序列的大量数据金矿，非常适合从自然语言处理

（NLP）中获取的语言模型（LM）。这些 LM 以较低的推理成本进入新的预测前沿。在这里，

我们训练了两个自回归模型（Transformer-XL，XLNet）和四个自动编码器模型（BERT，

Albert，Electra，T5），来自UniRef和BFD的数据包含多达393亿个氨基酸。蛋白质

LMs（pLMs）在Summit超级计算机上使用5616个GPU和TPU Pod进行训练，最多1024个内核。降

维表明，来自未标记数据的原始pLM嵌入捕获了蛋白质序列的一些生物物理特征。我们验证了

将嵌入作为几个后续任务的独占输入的优(cid:1294)：（1）蛋白质二级结构的每残基（每标记）预测

（3态准确度Q3=81%-87%）;（2）蛋白质亚细胞位置（十态精度：Q10=81%）和膜与水溶性（2

态精度Q2=91%）的每蛋白（池化）预测。对于二级结构，信息量最大的嵌入（ProtT5）首次优

于没有多序列比对（MSA）或进化信息的最新技术，从而绕过了昂贵的数据库搜索。综上所

述，结果表明pLM学习了一些生活语言的语法。我们所有的型号都可以通过

https://github.com/agemagician/ProtTrans 获得。

来(cid:14362) <https://pubmed.ncbi.nlm.nih.gov/34232869/>

