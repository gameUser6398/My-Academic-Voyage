分区 文献阅读 的第 48 页

[10012121]]
词袋模型虽然实现简单，并且比one-hot增加了词频的信息，但仍然存在(cid:13674)陷。由于词袋模型只是把句子看作单词的简单集
合，忽略了单词出现的顺序，可(cid:14125)导致顺序不一样的两句话在机器看来是完全相同的语义。

2.3. N-gram 模型
N-gram也是一种基于(cid:13583)计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了
长度是N的字节片段序列。每一个字节片段称为gram，对所有gram的出现频度进行(cid:13583)计，并且按照事先设定好的阈值进行过
滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量(cid:13604)度[3]。
该模型基于这样一种假设，第N个词的出现只与前面N- 个词相关，(cid:13884)与其它任何词都不相关，整句的概率就是各个词出现概率
的乘积。这些概率可以通过直接从语料中(cid:13583)计N个词同时出现的次数得到。当 N   时称为 unigram 模型即一元模型，也叫上
下文无关模型，上文提到的bow就是 unigram 模型；当 N 2 时称为 bigram 模型即二元模型；当 N 3 时称为 trigram 模型
即三元模型。
2.3.  实验
fromsklearn.feature_extraction.textimportCountVectorizerdefn_gram(texts):'''
    CountVectorizer：文本特征提取计算类，会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数
    '''vectorizer=CountVectorizer(ngram_range=(1,2))# ngram_range参数：词组切分的长度范围
texts=vectorizer.fit_transform(texts)# 拟合模型，并返回文本矩阵print(vectorizer.get_feature_names())# 获得所有文本的词
汇；列表型returntextstext=['AA is BB, and BB is AA','CC is not AA, but CC is DD']text=n_gram(text)# 此处text为csr_matrix
类型，是一个稀疏矩阵。如： 2  3     代表第二行第三列的值为 ，其余全为0。print(text.toarray())# 将csr_matrix转换为ndarray#
输出['aa','aa but','aa is','and','and bb','bb','bb and','bb is','but','but cc','cc','cc is','dd','is','is aa','is bb','is
dd','is not','not','not aa'][[20111211000002110000][11000000112212001111]]
N-gram 模型的基本原理是基于马尔可夫假设，在训练 N-gram 模型时使用最大似然估计模型参数——条件概率。当N更大的时
候，对下一个词出现的约束性信息更多，有更大的辨别力，但是更稀疏，并且N-gram的总数也更多；当N更小的时候，在训练
语料库中出现的次数更多，有更可靠的(cid:13583)计结果，更高的可靠性 ，但是约束信息更少。并且，N-gram模型无法避免零概率的
问题，导致无法获得良好的语言模型[4]。

2.4. TF-IDF 模型
前文提到的几种方法都是基于单词在文档中出现的频率来判断来猜测语义，也符合人类对于语言的理解规律。可是，出现频率
越大的词往往对于判断语义并没有实质性的帮助，例如“ ”，“是”，“的”，“今天”等词语。(cid:13884)像“足球”，“口
红”，“(cid:14033)票”等词则更(cid:14125)反应一篇文章的主题。
为了解决这个问题， 们提出两种解决方案：第一个是增加停用词（stop word），通过(cid:14362)定义词典，来去掉一些无用的高频
词；第二个就是本章要讲的TF-IDF 模型。
TF-IDF算法是一种可应用于多个领域的加权技术，它是基于(cid:13583)计的方法，根据某关键字或词在文档或语料集中出现的频率来估
计它对于文件的重要程度[5]。TF-IDF算法常常被用于信息检索任务中。算法的核心思想是，字词在文档中出现的次数越多，
其重要程度就越高，但它如果在语料集出现的次数越多，它的重要程度则会随之降低。
TF（Term Frequancy）代表词频，表示词在文档中出现的频率。IDF（Inverse Document Frequency）代表逆文档频率。TF值
和IDF值越高，则表示此词在一篇文档中出现概率高并且在其他文档中出现概率低，说明这个词具有良好的类别区分(cid:14125)力，应
赋予其更高的权重。
2.4.  实验
fromsklearn.feature_extraction.textimportTfidfVectorizerdeftfidf(texts):'''
    CountVectorizer：文本特征提取计算类，会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数
    '''vectorizer=TfidfVectorizer(ngram_range=(1,2))texts=vectorizer.fit_transform(texts)# 拟合模型，并返回文本矩阵
print(vectorizer.get_feature_names())# 获得所有文本的词汇；列表型returntextstext=['AA is BB, and BB is AA','CC is not AA,
but CC is DD']text=tfidf(text)# 此处text为csr_matrix类型，是一个稀疏矩阵。如： 2  3     代表第二行第三列的值为 ，其余全为
0。print(text.toarray())# 将csr_matrix转换为ndarray# 输出['aa','aa but','aa is','and','and bb','bb','bb and','bb
is','but','but cc','cc','cc is','dd','is','is aa','is bb','is dd','is not','not','not aa']
[[0.366811020.0.257770040.257770040.257770040.515540090.257770040.257770040.0.0.0.0.0.366811020.257770040.257770040.0.0.0
.]
[0.16528290.232299350.0.0.0.0.0.0.232299350.232299350.46459870.46459870.232299350.33056580.0.0.232299350.232299350.232299
350.23229935]]
通过计算词频和逆文本频率，TF-IDF 在(cid:13875)虑效率 的同时也得到了比较满意的效果。但由于 TF-IDF 仅仅(cid:13875)虑与词频相关的(cid:13583)
计，没有关注单词与单词之间的(cid:13956)系。与独热表示相同，TF-IDF 依然存在向量(cid:13604)度较高、不(cid:14125)准确表示文本语义的(cid:13674)点。
2.5. Word2Vec 模型
20 3年，Google 团队提出了开源的训练词嵌入向量的工具Word2vec[6]，它的核心思想是根据关键词去预测其上下文或根据关
键词的上下文去预测关键词，从某种意义上讲词的向量化表示是模型训练的副产物。 Word2vec 词嵌入模型(cid:14125)够很好的表示词
与词之间的类比关系和相似关系。Word2vec 模型包含两种结构：CBOW、skip-gram；和两种优化方法softmax、negative
sampling（本章不详细展开）。
2.5.1. CBOW（连(cid:13597)词袋）模型
CBOW continuous bag of words 模型的核心思想是：在一个句子中遮住目标单词，通过其前面以及后面的单词来推测出这个
单词w。首先规定词向量的(cid:13604)度V，对数据中所有的词随机赋值为一个V(cid:13604)的向量，每个词向量乘以参数矩阵W VN(cid:13604)矩阵 ，转换
成N(cid:13604)数据，然后要对窗口范围内上下文的词向量相加取均值作为输入层输入到隐藏层，隐藏层将(cid:13604)度拉伸后全连接至输出层
然后做 一个softmax的分类从(cid:13884)预测目标词。最终用预测出的w与真实的w作比较计算误差函数，然后用梯度下降调整参数矩
阵。
2.5.2. skip-gram（跳字）模型

分区 文献阅读 的第 49 页

2.5.2. skip-gram（跳字）模型
skip-gram模型的核心思想是：模型根据目标单词来推测出其前面以及后面的单词。它的模型结构与CBOW正好相反，只不过它
的输入是目标词，输出是目标词的邻接词，从模型结构示意图上看相当于输入层与输出层交换位(cid:13726)，先将目标词词向量映射到
投影层，再将投影层的输出作为输出层的输入，最后预测目标词窗口范围内的邻接词。

Word2Vec 的(cid:13674)点是，由于其训练出来的词嵌入向量表示与单词是一对一的关系，一词多义问题还是没有解决。单词在不同上
下文中是具有不一样含义的，(cid:13884) Word2Vec 学习出来的词嵌入表示不(cid:14125)(cid:13875)虑不同上下文的情况。 通过 Word2Vec 等技术方法
得到的静态的词嵌入表示，其本质上就是当模型训练好之后，在不同的上下文语境中，单词的词嵌入表示是一样的，不会发生
改变。
2.6. GloVe 模型
GloVe模型[7]是由斯坦福教授Manning、Socher等人于20 4年提出的一种词向量训练模型。上文讲的 Word2vec模型只(cid:13875)虑到了
词与窗口范围内邻接词的局部信息，没有(cid:13875)虑到词与窗口外的词的信息，没有使用语料库中的(cid:13583)计信息等全局信息，具有局限
性。GloVe模型则使用了(cid:13875)虑全局信息的共现矩阵和特殊的损失函数，有效解决了 Word2vec的部分(cid:13674)点。
2.6. . 共现矩阵
GloVe模型训练词向量的第一步就是根据语料库构建共现矩阵。所谓的共现就是看一个词有没有在另一个词的附近出现，就是
看一个词的移动窗口范围内出现词的次数。例如：
I like you.
I like NLP.
I love Knowledge Graph.
有以上三句话，设(cid:13726)滑动窗口为2，可以得到一个词典：{"I like" "like you" "like NLP" "I love" "love
Knowledge" "Knowledge Graph"}。
2.6.2. 损失函数
损失函数的基本形式是均方误差，在此基础上增加了
这一权重函数。公式如下所示：

•
•
•

其中，
和
是在初始阶段单词i和单词j的词向量，它们在后(cid:13597)的训练中会逼近真实的词向量，本质上它的训练方式跟监督学习的训练方法
相同，都是基于梯度下降的。
和
是为了增强模型的鲁棒性(cid:13884)增加的偏(cid:13726)项，
是根据共现矩阵得到的单词i和单词j共同出现在一个窗口 window 中的频数。
的作用是当词没有共现时，即
  0时使之不参与损失函数的计算；使高频单词权重减小；使过大的权重增长到一定程度后不再增长。
GloVe 模型相对于Skip-Gram和CBOW利用了词共现的信息，克服了 word2vec 模型只关注上下文局部信息的(cid:13674)点，在利用上下
文信息的同时使用了词共现的信息，有效地结合了全局的(cid:13583)计信息和局部上下文信息。
同时，Glove的优点是训练快，可以拓展到大规模语料，也适用于小规模语料和小向量，且最终效果通常更好。[8]
参(cid:13875)文献：

[ ]. (cid:13604)基百科(cid:13638)(cid:13877). 词嵌入[G/OL]. (cid:13604)基百科  202  202 05 7 [202 -05-17]. https://zh.wikipedia.org/w/index.php?
title=%E8%AF%8D%E5%B5%8C%E5%85%A5&oldid=65669330.
[2]. BE T发展史（一）从词嵌入讲起 - 蛋炒天津饭的文章 - 知乎 https://zhuanlan.zhihu.com/p/55219437
[3]. 文本表示之词袋模型 - 一个懒人的文章 - 知乎 https://zhuanlan.zhihu.com/p/53302305
[4]. 尹陈 吴敏. N-gram模型(cid:13612)述[J]. 计算机系(cid:13583)应用 20 8 27  0 :33-38. DOI:10.15888/j.cnki.csa.006560.
[5]. HE Xiaojing.Improvement and experimental research on TF-IDF algorithm[D].Changchun:Jilin University,2017
[6]. Mikolov T.，CHEN K.，Corrado G.，et al. Efficient Estimation of Word  epresentations in Vector Space[J].
arXiv Preprint: 30 .378 ， 20 3.
[7]. Pennington J, Socher R, Manning C. Glove: Global vectors for word representation[C]//Proceedings of the
2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.
[8].深度学习与(cid:14362)然语言处理（五）：Glove - 人工智(cid:14125)插班生的文章 - 知乎 https://zhuanlan.zhihu.com/p/58916233
发布于 202 -10-17 21:46

word embedding

机器学习

(cid:14362)然语言处理
赞同 292 条评论

分区 文献阅读 的第 50 页

赞同 292 条评论
分享
喜欢收藏申请转载

赞同 29

分享

评论千万条，友善第一条

2 条评论
默认
最新

学习烦躁

大致看了一下，写的很详细，希望后(cid:13597)(cid:14125)出更多的接地气的nlp内容，关注你咯。
2021-10-18
回复赞

Lewis
赞

2022-09-06
回复赞

推荐阅读

(cid:14362)然语言处理3： word embedding

前面在介绍tf-idf的时候讲过使用one-hot(cid:13638)码来表示一个单词。这是个比较简单直观的方法，(cid:13884)且计算机也确实很在行处理向
量。One-hot的意思，又称为一位有效(cid:13638)码，主要是采用N位状态寄存器来…

ustcsse308

Word Embedding News｜词嵌入新鲜事：COVID- 9特刊

 的土歪客发表于词嵌入杂谈

(cid:14362)然语言处理工具：中文 word2vec 开源项目，教程，数据集

中文 word2vec 开源项目 Chinese word vectors This project uses Word2vec and GloVe tools to train word vectors for
Chinese using data from wikipedia dump. https://github.com/cand…

灰灰发表于磐创AI

word2vec（三）：当 谈词嵌入时 谈些什么

 的土歪客发表于词嵌入杂谈

来(cid:14362) <https://zhuanlan.zhihu.com/p/422542949>

分区 文献阅读 的第 51 页

分区 文献阅读 的第 52 页

词向量构建
16:00

2023年5月24日

将氨基酸词库中的所有词训练成词向量表示，并构建词(cid:2125)表，通过索引的方式存储其特征向量。

如：FastText方法，每个由 3 个氨基酸 组成的词都以 1000 维的词向量表示。来自秦

分区 文献阅读 的第 53 页

Algorithms, Applications, and

Challenges of Protein Structure

Alignment（蛋白质结构比对的算

法、应用和挑战）

2023年6月7日

19:02