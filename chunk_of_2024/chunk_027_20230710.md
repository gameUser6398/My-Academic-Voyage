分区 文献阅读 的第 55 页

3.3 预处理

图片表示3个输入氨基酸序列的热编码过程

一个热编码为每个序列创建一个新维度。所有这些维度在向量空间中彼此正交。该向量的长度

等于序列的总数。如图所示，每个维度都表示为零的整数二进制向量，但在相应的序列索引处

除外，其中分配的值为 1。使用独热编码背后的方法是防止模型在训练过程中过度拟合行为

（Shorten和Khoshgoftaar，2019）。

在所提出的模型中使用一种热编码来表示输入层，以便对输入序列进行编码。在本文中，所提

出的模型可以接受每个输入蛋白质序列长达2000个氨基酸的长度。这样做是为了涵盖科学文

献中针对现实生活场景提出的大多数输入氨基酸序列。

3.4 神经网络构建块
3.4.1  池化 Pooling

池化是一种表示为层的技术，用于在不丢失输入特征细节的情况下对卷积层的维度进行下采

样。池化的两种主要类型：最大值和平均值。最大池化更适合提取极端特征，而平均池化有时

会考虑所有特征，从而产生非关键特征。在所提出的模型中，使用和实现了Max-Pooling类

型。

3.4.2 激活函数

为输出神经元增加了一些非线性，以允许模型学习复杂的任务。它根据激活函数的特定形状激

活/停用前一层的一些神经元。

图  Deep-CNN-LSTM-GO中使用的激活函数的绘制图，（a）“tanh”激活函数（B）

“Sigmoid”激活函数。

分区 文献阅读 的第 56 页

“Sigmoid”激活函数。

LSTM模型使用“tanh”激活函数，它说明了双曲正切函数：

或者，选择sigmoid函数作为整个系统的输出层：

3.4.3 展平层/扁平层

用于将卷积层的输出转换为单个长特征向量（一维数组）。执行此过程以将此向量馈送到输出

层。扁平化层可以有效地将学习过程加快一倍。

3.4.4 CNN

CNN是一种监督学习技术，可以将特征提取和特征分类过程合并到一个主体中。

由于输入数据是一维（1D）序列，因此在所提出的模型中使用1D CNN配置(cid:1710)方便，该配置

拟合具有不同大小内核的输入数据序列（Kiranyaz等人，2019）。 1D CNN只需要数组操

作，这意味着计算复杂度明显低于2D CNN。因此，1D CNN可以在任何标准硬件设置上进行

训练，并且不需要GPU。

3.4.5 长短期记忆 LSTM

长短期记忆（LSTM）是一种用于序列建模和NLP应用的递归神经网络架构。LSTM 是原始递

归神经网络 （RNN） 架构的增强版本，解决了无法访问RNN长期记忆的问题。此外，LSTM

解决了原始RNN架构所遭受的梯度消失问题。

LSTM 由三个主要部分组成：输入门、输出门和遗忘门。可以训练它来学习在内存中存储哪些

信息，存储多长时间以及何时读出。在所提出的模型中，使用了多个不同大小的LSTM。

4.Deep_CNN_LSTM_GO模型的拟实现

首先，蛋白质序列使用one-hot编码编码到基质中；对于 CNN，第一个参数是输入表示前一

层的输出，第二个参数是解释过滤器（内核）的数量，而第三个参数是过滤器（内核）长度或

大小。

对于池化，第一个参数是表示前一层输出的输入，第二个参数是解释池化层的长度或大小。

对于LSTM，第一个参数是输入，表示前一层的输出，第二个参数是解释输出单元的数量。

对于每个CNN模块，模型提取输入的局部特征，同时将这些具有一定非线性的输入映射到输

出。对每个区域重复此过程，这些区域使用过滤器数量和内核大小定义。

因此，CNN是一种神经网络架构，可以分层学习特征。换句话说，随着卷积层数量的增加，

可以学习高级的复杂特征。

另一方面，LSTM 是一种巧妙的技术，可以使神经网络决定要记住哪些重要信息以供进一步使

用，以及在序列中过滤和忘记哪些信息。利用这一优(cid:1294)，神经元可以从特征中学习并预测后续

值。这种组合使模型能够一次学习区域和时态特征。换句话说，卷积层检测“生物词”（区域

模式）和LSTM将它们绑定成具有“记住”或“忘记”特征的“生物句子”，然后将其馈送到

扁平层，生成GO表示的最终输出。如图11所示，说明了所提出的方法可以做什么的示例，显

示了输入和输出过程。

总结一下：

•

第一阶段：热编码是输入氨基酸序列的输入阶段，将每个氨基酸序列堆叠在单独的空间中，该

空间位于lndex= 1，其他为零。它采用单热编码，最大氨基酸序列输入长度为 2000 个不同

的符号。

•

第二阶段1D-CNN是卷积神经网络阶段。它由八个并行的 1D-CNN 架构组成。这些是使用以

下数量的过滤器构建和构建的：

分区 文献阅读 的第 57 页

下数量的过滤器构建和构建的：

2000−71992−71984−71976−71968−71960−71952−71944−7=

19931985197719691961195319451937
每个过滤器使用的最大内核大小（过滤器长度）为  28。

•

第三级最大池由八个最大池单元组成，连接到第二阶段的八个 1D-CNN 模块的输出。这些单

元的窗户大小为128。

•

第四级 LSTM 在每个最大池化单元之后分配。LSTM 级与尺寸为 8 的输出空间维度连接。它

通过“tanh”激活功能激活。

•

第五阶段展平来自 LSTM 级的所有输出都作为输入定向到构成该级的八个 Flatten 模块中的

每一个。

•

第六级输出从平坦级的八个模块的输出中获取输入。它包含使用Sigmoid激活函数激活的神经

元，以向输出添加非线性。输出神经元的数量等于所需函数的数量 4600。

综上，选择了 90% 的数据集来训练训练阶段的神经(cid:13697)络模型。权重首先随机初始化，
以防止输出神经元消失或爆炸。在训练步骤开始时，神经(cid:13697)络的性(cid:14125)最差。然后它开
始改进，并使用 ADAM 优化进行优化。系(cid:13583)使用损失函数评估性(cid:14125)，其中使用交叉熵
最小化该值以找到最佳精度。训练阶段从  024 个纪元开始，每个纪元有  605 个步
骤。在每个完成的 epoch 之后，将计算验证损失。然后使用 BGD 迭代更新优化过
程，直到损失函数最小化。

如果发生以下两种情况之一，则停止训练阶段。第一种情况是训练和验证损失函数在
6 个 epoch 后未优化。然后使用提前停止将使用最新的优化损失函数终止训练阶段。
第二种情况发生在纪元数完成时。

该过程完成后，使用 0%测试集计算最终损失函数。这是测试损失值。模型以检验损失
值 0.027383 结束。最终的训练和验证损失函数分别为 0.0359 和 0.0460。

5.评估指标
（1）精度

（2）召回率

（3）Fmax

（4）Smin

（5）AUPR

以上五种每一个都是针对三种蛋白质功能（MF,BP,CC）中的每一个计算的。

构建混淆矩阵：

（1）精度 Precision

（2）召回率 Recall

（3）Fmax

（4）Smin

分区 文献阅读 的第 58 页

（5）AUPR（精度-召回率曲线下的面积）

精度-召回率曲线显示不同阈值的精度和召回率之间的平衡。较高的 AUPR 是指精度和召回率

都(cid:1710)高。高精度与低假阳性率相关。或者，高召回率与低假阴性率有关。

6. 结果和生物学评价 Results and biological evaluation

6.1. UniProtKB-Swissport comparison (MF, BP, CC)

评估是针对该领域提出的三种不同方法进行的，GO-Labeler（You等人，2018a），

Deep_GO（Kulmanov和Khan，2018）和Deep_Go_Plus（Kulmanov和

Hoehndorf，2020）。如图 12 所示，使用 Fmax、Smin 和 AUPR 性能指标，将所提出的

方法与三个子本体（MF、BP、CC）中的每一个进行比较和测试。

分区 文献阅读 的第 59 页

Learned Embeddings from Deep Learning to Visualize and
Predict Protein Sets
2023年7月10日

19:30

从深度学习中学习嵌入以可视化和预测蛋白质集 - 达拉戈 - 2021 - 当前实验方案 - Wiley 在

线图书馆

基本协议 1：通用使用 bio_embeddings 管道绘制蛋

白质序列和注释

使用软件绘制蛋白质序列并通过属性对其进行着色

•

ntaining about 100 protein sequences in FASTA format

一个包含约100个FASTA格式的蛋白质序列

•

a CSV file containing DisProt (Hatos et al., 2020) classifications for these

sequences (whether their 3D structure presents mostly disorder or little
disorder)
一个包含DisProt的CSV文件（Hatos等人，2020） 这些序列的分类（无论它们的 3D 结

构主要呈现无序还是(cid:1710)少无序）

•

a configuration file that specifies parameters for the computation.

指定计算参数的配置文件

•

install the bio_embeddings software, executing the computation is a single step.

The following basic protocols present greater detail about the technical aspects

surrounding inputs, outputs and parameters of the pipeline.

我们在执行此协议时获得的输出可用于比较

http://data.bioembeddings.com/disprot/disprot_sampled;

执行步骤生成的打印文件可在

http://data.bioembeddings.com/disprot/disprot_sampled/plotly_visualization/plot_fil

e.html.

注意：此可视化是为一小部分DisProt序列样本生成的;因此，它绝不代表嵌入在区分DisProt类方

面的力量。

……

基本实验方案 2：使用 bio_embeddings 管道从蛋白

质序列生成嵌入

通过该协议，您可以使用bio_embeddings管道的“嵌入”阶段从一组蛋白质序列生成机器可读

分区 文献阅读 的第 60 页

通过该协议，您可以使用bio_embeddings管道的“嵌入”阶段从一组蛋白质序列生成机器可读

的表示（嵌入）。

在此协议中，我们使用BERT接受过BFD培训从蛋白质序列中提取嵌入物。该模型是ProtTrans蛋白

LMs的一部分，在文本中称为 ProtBERT，在以下代码中称为prottrans_bert_bfd。

嵌入阶段的主要输出是嵌入文件。它们有两种口味：每残基（embeddings_file.h5）和每蛋白质

（reduced_embeddings.h5）。虽然每个残基包埋直接从LM中取出，但每个蛋白质嵌入是通过全局

平均池化对LM提取的信息进行后处理生成的（Shen等人，2018） 在序列的所有组合每个残基嵌入

上。每残基包埋可用于分析蛋白质中残基的特性（例如，哪些残基结合配体），而每蛋白表示捕

获描述整个蛋白质的注释（例如，天然定位）。

基本协议 3：在蛋白质空间可视化上叠加序列注释

之前的协议从数据集中的蛋白质序列生成嵌入（此处为 DeepLoc 数据集）。在基本协议中3您可

以使用 bio_embeddings 包中的函数来可视化提取的嵌入所跨越的“蛋白质空间”。这些可视化

揭示了是否为“嵌入”阶段选择了LM（基本协议）2） 可以根据所需的属性/表型大致分离数据。

我们示例中的属性/表型是 10 个状态的亚细胞位置。备用协议2使用相同的数据和类似的步骤来

可视化蛋白质溶解度。虽然可视化(cid:1710)有用，但通过在嵌入上训练机器学习模型来预测所需的属

性，可以多次提高嵌入的辨别能力（基本协议）4).

在嵌入生成和蛋白质空间可视化之间，必须插入另一个步骤。在管道中，我们将此步骤称为“项

目”阶段。其目的是降低嵌入的维数（例如，ProtBERT 为 1024），以便可以在 2D 或 3D 中可

视化。在这里，我们将嵌入投影到 2D 上;备用协议1改用相同的数据，并在参数上略有变化来代

替 3D 图。

此处构建的最终笔记本可在以下位置获得：http://notebooks.bioembeddings.com作

为 deeploc_visualizations.ipynb 在本地下载和执行，或直接在线执行。该文件还包括备用协

议中介绍的步骤1和2.

这支持协议1 说明如何将最终可视化选项集成到配置文件中，作为管道管理整个过程（从序列到

可视化）的指令。这对于使同事能够从几个文件中重现所有结果非常有用。

分区 文献阅读 的第 61 页

基本协议 4：在蛋白质嵌入上训练机器学习分类器

基本协议2在DeepLoc中生成蛋白质嵌入（Almagro Armenteros等人，2017).

基本协议3在 2D 图中可视化投影嵌入，并通过表示亚细胞位置的颜色注释该 2D 图中的蛋白质。

在以下步骤中，您将使用通过管道生成的嵌入和 DeepLoc 中的位置注释来机器学习蛋白质序列嵌

入的位置预测。训练后，您可以应用此预测方法来注释/预测任何蛋白质序列的位置。构建通用机

器学习模型的最简单方法如下：

•

1.将数据划分为训练集和测试集（它们应该是序列非冗余的，即一个蛋白质序列中的任何一

个序列都不应该比另一个中的任何蛋白质的某个阈值更相似;这个阈值是什么取决于你的任

务）

2.从训练集中拆分子集以构造验证集（非冗余到拆分）

3.使用验证集评估一些机器学习超参数（例如，哪种类型的机器学习模型（例如 ANN、CNN

或 SVM），哪些特定的参数选择（例如 ANN/CNN 的隐藏单元/层数）。构建排行榜（即跟踪

所有模型/超参数的相对性能的表格）。

4.从排行榜中选择最佳模型，并在测试集上进行评估（绝不是将所有模型应用于测试集并选

择最佳模型;相反，必须使用验证集选择最佳模型并坚持该选择以避免过度拟合）。

5.使用测试集报告最终模型的各种相关评估指标的性能（包括对标准误差的估计）