分区 文献阅读 的第 47 页

编码方式

2023年5月22日

16:50



k-mer与n-gram
k-mer：它表示以 K 个 氨基酸为一个窗口，以步长为 1 的方式依次滑动，将整个核酸或者氨基 酸序列以 3-mer 的方法存储

下来。长度为 L 的氨基酸序列 被切分为 L-K+1 个词。从而构建一个氨基酸词库（语料库）

N-gram：将文本内容以 N 个字符的长度作为窗口依次滑动提取字符，这与基因测序的 k-mer 方法的思想相同。

基于k-mer的改动有：

-

-

窗口大小。在蛋白质信息学的n-gram建模中，通常使用3至6个残基的重叠窗口。
来(cid:14362) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716/>

重叠与否。

从0开始词嵌入（Word embedding）

2. 词嵌入方法
2.1. One-Hot（独热(cid:13638)码）模型
在最初NLP任务中，非结构化的文本数据转换成可供计算机识别的数据形式使用的是独热(cid:13638)码模型 one-hot code ，它将文本
转化为向量形式表示，并且不局限于语言种类，也是最简单的词嵌入的方式。
One-Hot将词典中所有的词排成一列，根据词的位(cid:13726)设计向量，例如词典中有m个词 则每个单词都表示为一个m(cid:13604)的向量，单词
对应词典中的位(cid:13726)的(cid:13604)度上为 ，其他(cid:13604)度为0。如图2.  所示，将一个句子中的每个“字”作为“词”表示，将得到每个
“词”的向量表示，由于一共有5种词，所以将它们映射为5(cid:13604)的向量。
2. . . 实验
fromsklearn.feature_extraction.textimportCountVectorizerdefone_hot(texts):'''
    CountVectorizer：文本特征提取计算类，会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数
    '''vectorizer=CountVectorizer(analyzer="char",binary=True)texts=vectorizer.fit_transform(texts)# 拟合模型，并返回文本
矩阵returntextstext=['东','北','大','学','在','东','北']text=one_hot(text)# 此处text为csr_matrix类型，是一个稀疏矩阵。
如： 2  3      代表第二行第三列的值为 ，其余全为0。text# 输出(0,0)1# [  0 0 0 0] 东(1,1)1# [0   0 0 0] 北(2,3)1#
[0 0 0   0] 大(3,4)1# [0 0 0 0  ] 学(4,2)1# [0 0   0 0] 在(5,0)1# [  0 0 0 0] 东(6,1)1# [0   0 0 0] 北
该方法虽然简单，并且适用于任意文本数据，但存在很多严重问题：
(cid:13604)度爆炸。由于每一个单词的词向量的(cid:13604)度都等于词汇表的长度，对于大规模语料训练的情况，词汇表将异常庞大，使模型的
计算量剧增造成(cid:13604)数灾难。
矩阵稀疏。有用的信息零散地分布在大量数据中。这会导致结果异常稀疏，使其难以进行优化，对于神经(cid:13697)络来说尤其如此。
向量正交。由于两两向量正交，无法表达两词向量之间的其他信息，造成了“语义鸿沟”的 现象，此特点对于NLP任务是相当
致命的。
所以，One-Hot只是简单地将“词”进行了(cid:13638)号，并没有表达词语的含义，并不符合语言的(cid:14362)然规律。那么，如何(cid:14125)使一个词
向量表达出更丰富的语义信息呢？
对于一个 们不理解的单词，如果 们知道了它在不同的上下文中是如何使用的， 们就(cid:14125)理解它的意思。根据经验，出现在
相似上下文语境中的词语有相似的含义。所以 们可以将词语的上下文语境信息放入到词向量中，也就说明 们获得了这个词
语的语义。获取上下文信息一般有两种方式，一种是基于计数的，一 种是基于预测的。接下来，本文将分别对这两种方式对
应的词嵌入方法展开介绍。

1.

2.
3.

2.2. Bag of Words（词袋表示）模型
词袋模型（Bow，Bag of Words），是文本向量化的一个模型，这种模型不(cid:13875)虑语法、词的顺序，只(cid:13875)虑所有的词的出现频
率，简单说，就是分好的词放到一个袋子中，每个词都是独立的。
向量的(cid:13604)度根据词典中不重复词的个数确定，向量中每个元素顺序与原来文本中单词出现的顺序没有关系 与词典中的顺序一
一对应，向量中每个数字是词典中每个单词在文本中出现的频率---即词频表示。
2.2.  实验
fromsklearn.feature_extraction.textimportCountVectorizerdefbow(texts):'''
    CountVectorizer：文本特征提取计算类，会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数
    '''vectorizer=CountVectorizer()texts=vectorizer.fit_transform(texts)# 拟合模型，并返回文本矩阵
print(vectorizer.get_feature_names())# 获得所有文本的词汇；列表型returntextstext=['AA is BB, and BB is AA','CC is not AA,
but CC is DD']text=bow(text)# 此处text为csr_matrix类型，是一个稀疏矩阵。如： 2  3     代表第二行第三列的值为 ，其余全为
0。print(text.toarray())# 将csr_matrix转换为ndarray# 输出['aa','and','bb','but','cc','dd','is','not'][[21200020]
[10012121]]

