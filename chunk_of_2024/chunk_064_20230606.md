分区 基础知识 的第 145 页

BP神经网络

•

•

•

•

深度学习属于机器学习的一部分，效果更好

视为一种特征提取的方法

流程：  数据获取

特征工程

建立模型

评估与应用

数据特征决定模型上限，因此核心在于预处理和特征提取，而算法参数选

择只是决定逼近上限的方式

机器学习

计算机视觉，文本数据还要建模

相比于决策树，参数量极大，几千万，几亿，因此速度(cid:1710)慢，但是准确

•

典型神经网络结构

分区 基础知识 的第 146 页

输入层和输出层的节点数往往是固定的，中间层改变

每个连接线对应不同的权重（训练得到，决定了预测效果），每个有向箭头表示值的加权传

递。

一个神经元（单元/节点）可以引出多个代表输出的有向箭头，但值都是一样的。

分区 基础知识 的第 147 页

一个神经元（单元/节点）可以引出多个代表输出的有向箭头，但值都是一样的。

这个式子属于线性代数方程组，可以使用矩阵乘法

•

两层神经网络

在输入层和输出层外，增加了中间层，后两者都是计算层

依然使用矩阵计算

ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)

在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数

g。

分区 基础知识 的第 148 页

g。

函数g也称作激活函数

小问题：

输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的

节点数，却是由

设计者指定的，应该如何指定？

A:没有一个指定标准，最好采用网络搜格【

预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的
值作为最终选择】

•

多层神经系统

优点：1. 相同层数，可以用更多参数，

分区 基础知识 的第 149 页

            2. 相同参数，更多层数，特征更加深入，拟合能力更强

最流行的非线性函数是ReLU函数【一种分段线性函数y=max(x,0)，在x大于0，输出就是输

入，而在x

小于0时，输出就保持为0。类似于生物神经元对于激(cid:1291)的线性响应，以及当低于某个阈值后

就不再

响应】

其他概念

损失函数

           损失函数是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实

值函数

使用主要是在模型的训练阶段，每个批次的训练数据送入模型后，通过前向传播输出预测

值，然后损失函数会计算出预测值和真实值之间的差异值，也就是损失值。得到损失值之

后，模型通过反向传播去更新各个参数，来降低真实值与预测值之间的损失，使得模型生成

的预测值往真实值方向靠拢，从而达到学习的目的。

损失函数=数据损失（经验风险：指由于拟合结果和样本标签之间的残差总和所产生的经验

性差距所带来的风险）+正则化惩罚项

为了使得模型不要太复杂

常见的基础代码：

常见损失函数 & 损失函数选择方法_Donald Su的博客-CSDN博客

损失函数（Loss Function） - 知乎 (zhihu.com)

选择：

对于二分类问题，可以使用二元交叉熵（binary crossentropy）损失函数；

分区 基础知识 的第 150 页

对于二分类问题，可以使用二元交叉熵（binary crossentropy）损失函数；

对于多分类问题，可以用分类交叉熵（categorical crossentropy）损失函数；

对于回归问题， 可以用均方误差（mean-squared error）损失函数；

对于序列学习问题，可以用联结主义时序分类（CTC，connectionist temporal classification）

损失函数，等等。

过拟合

模型过于专注于特定的训练数据而错过了要点，那么该模型就被认为是过拟合

直观表现是算法在训练集上表现好，但在测试集上表现不好，泛化性能差。过拟合是在模型

参数拟合过程中由于训练数据包含抽样误差，在训练时复杂的模型将抽样误差也进行了拟合

导致的。所谓抽样误差，是指抽样得到的样本集和整体数据集之间的偏差。

可(cid:14125)原因有：
1.模型本身过于复杂，以至于拟合了训练样本集中的噪声。此时需要选用更简单的模型，或
(cid:13877)对模型进行裁剪。
2.训练样本太少或(cid:13877)(cid:13674)乏代表性。此时需要增加样本数，或(cid:13877)增加样本的多样性。
3.训练样本噪声的干扰，导致模型拟合了这些噪声，这时需要剔除噪声数据或(cid:13877)改用对噪声
不敏感的模型。

模型的泛化误差来自于偏差和方差，前者是模型自身问题，代表预测值和实际值的

差距（高偏差产生欠拟合）

后者属于对于训练样本集的小波动敏感而导致的误差，属于预测值的波动程度，或者称为离

散值的分散程度。高方差意味着算法对训练样本集中的随机噪声【机器学习进行随机

独立抽样时出现的不合适信息】进行建模，从而出现过拟合问题

总体误差：

分区 基础知识 的第 151 页

如果模型过于简单，一般会有大的偏差和小的方差；反之如果模型复杂则会有大的方差但偏

差(cid:1710)小。这是一对矛盾，因此我们需要在偏置和方差之间做一个折中。

如何解决？

对于数据，分成三类，训练集，验证集和测试集

【训练数据用来训练模型；验证集用于在每一步测试构建的模型；测试集用于最后评

估模型。通常数据以 80:10:10 或 70:20:10 的比率分配。】

正则化L1,L2     看不懂，不想看，我哭死，线代gub

辛苦辣~

分区 基础知识 的第 152 页

https://zhuanlan.zhihu.com/p/161689789，

对于数据inputs = [batch_size,  words_size, hidden_size],

卷积层torch的in_channel值应该对应hidde_size, 需要对inputs.permute(0, 2, 1)

但是对于tensorflow，in_channel值无需设置，直接conv1d(inputs)

偶数卷积核大小(even-sized)与填充(padding)的副作用 - Drajun - 博客园 (cnblogs.com)

卷积神经网络

2023年6月6日

12:45