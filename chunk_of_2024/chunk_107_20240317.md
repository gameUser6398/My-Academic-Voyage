分区 已有模型 的第 299 页

DCGAN
2024年3月17日

23:40

DCGAN-tensorflow/model.py at master · carpedm20/DCGAN-tensorflow (github.com)

sess: TensorFlow session
      batch_size: The size of batch. Should be specified before training.
      y_dim: (optional) Dimension of dim for y. [None]
      z_dim: (optional) Dimension of dim for Z. [100]
      gf_dim: (optional) Dimension of gen filters in first conv layer. [64]
      df_dim: (optional) Dimension of discrim filters in first conv layer. [64]
      gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]
      dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]
      c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]

# lrelu(batch_norm(conv2d())) + linear() + sigmoid()
  def discriminator(self, image, y=None, reuse=False):
    with tf.variable_scope("discriminator") as scope:
      if reuse:
        scope.reuse_variables()

      if not self.y_dim:
        h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))
        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1
_conv')))
        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2
_conv')))
        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3
_conv')))
        h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_lin')
【TensorFlow】理解tf.nn.conv2d方法 ( 附代码详解注释 )-CSDN博客

        return tf.nn.sigmoid(h4), h4

    else:
        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])
        x = conv_cond_concat(image,  yb) # 输入时合并img和y

        h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0
_conv'))
        h0 = conv_cond_concat(h0, yb)

        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim,
name='d_h1_conv')))
        h1 = tf.reshape(h1, [self.batch_size, -1])
        h1 = concat([h1, y], 1)

        h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))
        h2 = concat([h2, y], 1)

        h3 = linear(h2, 1, 'd_h3_lin')

        return tf.nn.sigmoid(h3), h3

# relu(linear()) + deconv2d() + tanh()

分区 已有模型 的第 300 页

# relu(linear()) + deconv2d() + tanh()

# 从latent_size --> 1024  --> 反卷积 reshape

def generator(self, z, y=None):

    with tf.variable_scope("generator") as scope:
      if not self.y_dim:
        s_h, s_w = self.output_height, self.output_width
        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w,
2)
        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2,
2)
        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4,
2)
        s_h16, s_w16 = conv_out_size_same(s_h8, 2),
conv_out_size_same(s_w8, 2)

        # project `z` and reshape
        self.z_, self.h0_w, self.h0_b = linear(
            z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)

        self.h0 = tf.reshape(
            self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])
        h0 = tf.nn.relu(self.g_bn0(self.h0))

        self.h1, self.h1_w, self.h1_b = deconv2d(
            h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1',
with_w=True)
        h1 = tf.nn.relu(self.g_bn1(self.h1))

【TensorFlow】tf.nn.conv2d_transpose是怎样实现反卷积的？-CSDN博客

tf.nn.conv2d_transpose反卷积（转置卷积）-CSDN博客

ConvTranspose2d原理，深度网络如何进行上采样？_dwconvtranspose2d-CSDN博客

   return tf.nn.tanh(h4)

else:

        s_h, s_w = self.output_height, self.output_width
        s_h2, s_h4 = int(s_h/2), int(s_h/4)
        s_w2, s_w4 = int(s_w/2), int(s_w/4)

        # yb = tf.expand_dims(tf.expand_dims(y, 1),2)
        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])
        z = concat([z, y], 1) # 输入时合并zy

        h0 = tf.nn.relu(
            self.g_bn0(linear(z, self.gfc_dim=1024, 'g_h0_lin')))
        h0 = concat([h0, y], 1) # 合并y

        h1 = tf.nn.relu(self.g_bn1(
            linear(h0, self.gf_dim*2*s_h4*s_w4=64, 'g_h1_lin')))
        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim
* 2])

        h1 = conv_cond_concat(h1, yb) # 合并y

        h2 = tf.nn.relu(self.g_bn2(deconv2d(h1,
            [self.batch_size, s_h2, s_w2, self.gf_dim * 2],
name='g_h2')))
        h2 = conv_cond_concat(h2, yb) # 合并y

分区 已有模型 的第 301 页

        return tf.nn.sigmoid(
            deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim],
name='g_h3'))

def sigmoid_cross_entropy_with_logits(x, y):

      try:
        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)
      except:tf.nn.sigmoid_cross_entropy_with_logits详解-CSDN博客
        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x,
targets=y)

self.d_loss_real = tf.reduce_mean(
      sigmoid_cross_entropy_with_logits(self.D_logits,
tf.ones_like(self.D)))
self.d_loss_fake = tf.reduce_mean(
      sigmoid_cross_entropy_with_logits(self.D_logits_,
tf.zeros_like(self.D_)))
self.g_loss = tf.reduce_mean(
      sigmoid_cross_entropy_with_logits(self.D_logits_,
tf.ones_like(self.D_)))

self.d_loss_real_sum = scalar_summary("d_loss_real", self.d_loss_real)
self.d_loss_fake_sum = scalar_summary("d_loss_fake", self.d_loss_fake)

self.d_loss = self.d_loss_real + self.d_loss_fake   # 本质相同

分区 已有模型 的第 302 页

PO2GO
2024年5月11日

18:06

import torch

from torch import nn

from torch.nn import BCEWithLogitsLoss
import torch.nn.functional as F

class PO2GO(nn.Module):
    def __init__(self,
                 terms_emb,
                 protein_dim=1280,
                 latent_dim=768,
                 prob_predict_temp_dim=1280
                 ):
        super().__init__()
        self.terms_emb = terms_emb
        self.nb_classes = terms_emb.shape[0]
        self.latent_dim = latent_dim
        self.term_dim = terms_emb.shape[1]
        self.protein_dim = protein_dim

        # protein projector
        self.q = self.init_predictor(self.protein_dim,
                                     self.latent_dim, 1024, 1024, 0.2)
        # term projector
        self.k = nn.Linear(self.term_dim, self.latent_dim)
        # probility predictor
        self.fc = nn.Linear(self.nb_classes, prob_predict_temp_dim)
        self.fc2 = nn.Linear(prob_predict_temp_dim, self.nb_classes)

    def init_predictor(self, in_dim, out_dim, temp_dim=1024, norm_dim=1024, drop=0.):
        return nn.Sequential(
            nn.Linear(in_dim, temp_dim),
            nn.BatchNorm1d(norm_dim),
            nn.ReLU(),
            nn.Dropout(drop),
            nn.Linear(temp_dim, out_dim),
        )

    def forward(self, embeddings, labels=None):
        x = self.q(embeddings)
        y = self.k(self.terms_emb)
        x = torch.matmul(x, y.T)
        x = self.fc(x)
        x = F.relu(x)
        x = F.dropout(x, 0.)
        logits = self.fc2(x)
        outputs = (logits, )
        if labels is not None:
            loss_fct = BCEWithLogitsLoss()

分区 已有模型 的第 303 页

            loss_fct = BCEWithLogitsLoss()
            labels = labels.float()
            loss = loss_fct(logits.view(-1, self.nb_classes),
                            labels.view(-1, self.nb_classes))
            outputs = (loss, logits)

        return outputs

if __name__ == '__main__':
    node_embedding = torch.rand((10000, 200))
    protein_embedding = torch.rand((4, 1280))
    label = torch.zeros((4, 10000))
    model = PO2GO(terms_emb=node_embedding,
                  protein_dim=1280,
                  latent_dim=768,
                  prob_predict_temp_dim=1280)
    outs = model(protein_embedding, label)

分区 已有模型 的第 304 页

Transformer参数及结果记录

2023年7月15日

22:07