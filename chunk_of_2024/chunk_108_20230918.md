

8月27日版

结果：loss从160开始，在160附近大幅震荡。

8月28日版

修改d_queries, d_values

对初始化做了修改

结果：

8月29日

将decoder的softmax除去，因为调用crossentropy时会自动调用softmax



分区 模型优化过程 的第 305 页

1.

2.

将decoder的softmax除去，因为调用crossentropy时会自动调用softmax

对tensorboard绘图作了修改

8月30日
1.

受“Transformer如何用于分类”页上的图片启发，去掉编码层

2.

拿每个序列的最后一位输出去预测（失败）

分区 模型优化过程 的第 306 页

TALE-repetition-1
2023年9月18日

23:54

在label——embedding时，格式是否为

9月21日

loss使用了binary_cross_entropy，处理的是sigmoid

f1-score不断下滑，猜想：这与没有经过softmax有关，只经过了sigmoid（错误

修改：在model末尾加softmax，使用binary_cross_entropy, 和f1-score

结果：更差。看来还是得用sigmoid

分区 模型优化过程 的第 307 页

都用sigmoid

分区 模型优化过程 的第 308 页

对TALE实现端对端预测
1:59

2023年10月23日

10月23日

主要改动：

1.

实现了data_load_from_scratch，从fasta、terms.tsv文件获取原始信息输入模型

2.

3.

4.

利用pickle库，实现了断点续存。节省了调试时间

解决了UserWarning: Using padding='same' with even kernel lengths and 的报错

使用dense_matrix对label进行编码，nn.embedding

参数设置：

TODO
1.

测试batch_size的影响，研究batch_effect是否可以通过同源序列比较。

2.

3.

对于TALE的label_embedding，转换为【0,1】稀疏矩阵后，它对0,1进行嵌入有什么意

义？（维度扩充了但还是只有两种，还是0与1的区别？

现在得到label的密集矩阵，如果对[3, 24, 345]这样的密集矩阵进行嵌入，是不是把

label它当成了一个句子，这时候，label需要根据它的网络，进行深度优先处理吗？

结果：平滑值0.8

分区 模型优化过程 的第 309 页

-

改动1：
1.

label_coding使用0,1的稀疏矩阵，

未成功

10月25日

改动：

1.

调小卷积核【CNN基础】为什么要用较小的卷积核_卷积核大小对模型的影响-CSDN博客，

conv1-kernal_size = 5。

直觉，loss更大，运算时间更长。因为更稀疏？

池化后的joint_similaritey

CAFA训练集GOterms的数据特征：5的倍数、2的倍数

分区 模型优化过程 的第 310 页

皓哥我调小了kernel_size，增多了层数，但效果不明显。除了黄线（我在两层cnn基础

上，仅仅调小了第一层的kernel_size）效果明显离谱外，其它的不明显，看来还是

label_embedding的问题（我想验证网络是否能用语言表示）。

网络能否用语言表示？

10月26日

从计算时间上看，一个蛋白不能超过0.4s的时间，如果CAFA训练三个世代，那么训练完需要

整整花费48小时。

11月5日

调大batch_size=64，

分区 模型优化过程 的第 311 页

TALE-11-8-focalloss
2023年10月23日

11:52

Focal Loss 的Pytorch 实现以及实验 - 知乎 (zhihu.com)

Focal Loss损失函数（超级详细的解读）-CSDN博客

pytorch-loss/focal_loss.py at master · CoinCheung/pytorch-loss (github.com)

11-8启动，11-11结束，训练5个epoch

Focal loss在训练过程中不断增大可(cid:14125)是由于以下原因：
学习率设(cid:13726)过高：学习率是梯度下降中的一个重要参数。如果学习率设(cid:13726)得过高，可(cid:14125)会导
致损失函数在优化过程中”跳过”优异点，从(cid:13884)造成损失函数不断震荡并逐渐增大。

数据问题：数据中的噪声、异常值或不均匀的分布可(cid:14125)会导致模型的损失函数上升。此外，
如果训练数据和验证数据分布不一致，也可(cid:14125)导致损失逐渐增大。

(cid:13697)络结构问题：模型过于复杂或过于简单都可(cid:14125)导致损失增大。过复杂的模型可(cid:14125)会过拟
合，(cid:13884)过于简单的模型可(cid:14125)不(cid:14125)捕获数据中的复杂关系。

初始化问题：不合适的权重和偏(cid:13726)初始化可(cid:14125)导致训练过程中损失函数快速增大。例如，将
所有权重初始化为0可(cid:14125)会导致梯度消失问题。

正则化过强：正则化是一种防止过拟合的技术，但如果正则化强度过大，可(cid:14125)会抑制模型的
(cid:14125)力，导致损失增大。

其他外部因素：例如，硬件问题、数据读取错误或其他外部干扰都可(cid:14125)影响训练过程，导致
损失函数的异常变化。

为了解决这些问题，可以尝试以下方法：
动态改变学习率：例如使用Adam等优化算法。

检查数据质量：移除噪声和异常值，确保训练和验证数据的分布一致。

调整(cid:13697)络结构：根据实际问题需求，选择合适复杂度的模型。

1.

2.

3.

4.

5.

6.

7.

8.

9.

10.

合适的初始化策略：例如使用Xavier初始化或(cid:13877)He初始化。

11.

调整正则化强度：找到适合模型的平衡点，避免正则化过强。

12.

检查硬件和外部因素：确保硬件问题和外部干扰不会影响训练过程。

来(cid:14362) <https://yiyan.baidu.com/>

分区 模型优化过程 的第 312 页

11-11观察，Epoch=4

参数初始化？

optimizer.zero_grad()？

分区 模型优化过程 的第 313 页

TALE-11-9-hiddensize1
23:54
2023年11月9日

尝试将anc2vec（最大、最小、平均）池化、然后聚类比较，

利用T5、anc2vec嵌入结果

分区 模型优化过程 的第 314 页

WGAN损失函数记录
16:19

2024年3月14日

3-14原始

3-15

分区 模型优化过程 的第 315 页

-

discriminator减少了一个1024-1024的Linear，

0321-14

分区 模型优化过程 的第 316 页

来(cid:14362) <http://localhost:8080/lab/tree/imblearn/WGAN/wgan_V3_FullyConnected.ipynb>

分区 模型优化过程 的第 317 页

CGAN-V1
2024年3月16日

11:11

1.

2.

3.

损失函数

latent_space大小

Linear层数多少

3-16 原始，有sigmoid

去掉sigmoid()

分区 模型优化过程 的第 318 页

**V1 3.16.3**拿掉sigmoid，latent_space设为256

结论：需要更多的layer，少量的latent_space

分区 模型优化过程 的第 319 页

结论：需要更多的layer，少量的latent_space

latent_space设为64

重复实验

分区 模型优化过程 的第 320 页

CGAN-V2
2024年3月17日

11:55

1.

2.

数据量的增大

n_critic作用

原始模型：embeddding使用num20-3000

结论：opt.n_critic过小（每个iteration内，对discriminator只有batch_size * opt.n_critic个

训练样本

使用num20-200，n_critic为3

分区 模型优化过程 的第 321 页

使用num20-200，n_critic为5

分区 模型优化过程 的第 322 页

CGAN-V3
2024年3月17日

22:48

精巧的改造达到了惊艳的效果——WGAN+pytorch代码实现及代码详解 - 知乎 (zhihu.com)

1.

2.

3.

4.

用卷积

输入时，合并y标签

再试试sigmoid

尝试加入go-embed

无sigmoid，无卷积，只+1合并y标签

减小n_critc为3，

-

添加两层selfAttention，一共3层layer。太慢了

0319-13来(cid:14362) <http://localhost:8080/lab/tree/imblearn/WGAN/cgan_V3_add_go_embd.ipynb >

4月1日，V3

4月1日，V3，感觉400个epoch比较好

0319-18使用转置卷积
来(cid:14362) <http://localhost:8080/lab/tree/imblearn/WGAN/cgan_V3_SelfAttention_Conv.ipynb >

0319-21 。

2400个epoch后：
D_loss: tensor(-0.1494)
G_loss: tensor(0.1386)

初步结论

self-ATTention更好。未训练结束，epoch=23，
0320-19 http://localhost:8080/files/imblearn/WGAN/cgan_V3_SelfAttention_Conv.ipynb?_xsrf=2%
7C01178cd1%7C65c2482b11b27b95053683ce1e894d02%7C1710674787

分区 模型优化过程 的第 323 页

CGAN使用全连接层

0321-09

分区 模型优化过程 的第 324 页

左列：tsne降维。右列：UMAP降维

上下四行分别为：

0319- 3，V3_CGAN-selfAttention-epoch500

0401，V3_CGAN-selfAttention-epoch400

0401，V3_CGAN-selfAttention-epoch600

0401，V3_CGAN-selfAttention-epoch800

结论：

1.

umap倾向于保存全局结构，“距离度量不是

在整个空间中通用的，而是在不同区域之间变

化的。”

2.

tsne保存局部结构，缩小了偏远数据的离散程

度

3.

从tsne图看，GAN生成的结果总是“中间数

据”，填充向量空间的空隙。

4月2日训练/降维结果
18:58

2024年4月2日

分区 模型优化过程 的第 325 页

mf
2024年4月8日

16:49

4222行（protTotal (20, 200)   )， 14342行（protTotal (200-2000）都是开区间。合计18564

真实蛋白的余弦相似度阈值，这个阈值是在【real_embd, gene_embd】两部分数据上投影、计

算

以上数据可能有误！！！

0409-05

0409-07

？？？0409-05

0409-07

/mnt/data/liuchangle/mf-embd-cgan-resample/V3_for_num20-1000_400_0409/

0409-08

数据存在：+ /data/gen_prots_0409-08.csv，用来

0409-08

分区 模型优化过程 的第 326 页

'/mnt/data/liuchangle/mf-embd-cgan-resample/V3_for_num20-1000_600_0409/'

'gen_prots_0409-1041.csv

来(cid:14362) <http://localhost:8889/lab/tree/Protein-imblearn/embed-CGAN/cgan_V3_SelfAttention_Conv.ipynb>

来(cid:14362) <http://localhost:8889/lab/tree/Protein-imblearn/Cluster_evaluation/UMAP-visual-
resampled.ipynb>

'/mnt/data/liuchangle/mf-embd-cgan-resample/V3_for_num20-2000_1200_0410/
0410-1134

来(cid:14362) <http://localhost:8889/lab/tree/Protein-imblearn/embed-CGAN/cgan_V3_SelfAttention_Conv.ipynb>

'

分区 模型优化过程 的第 327 页

cc
2024年4月8日

16:49

4256行（20-200），9550行（200-2000）

discriminator_param_0409-07.pkl、discriminator_param_0409-08.pkl、

分区 模型优化过程 的第 328 页

来(cid:14362) /mnt/data/liuchangle/cc-embd-cgan-resample/V3_for_num20-200_400_0409
0409-0847
gen_prots_0409-0847

来(cid:14362) <http://localhost:8889/lab/tree/Protein-imblearn/embed-CGAN/cgan_V3_SelfAttention_Conv.ipynb>

分区 模型优化过程 的第 329 页

bp
2024年4月8日

16:49

分区 模型优化过程 的第 330 页

嵌入质量控制

2024年4月7日

22:21