分区 项目进展 的第 35 页

10-31-汇报

2023年10月31日

12:52



TALE是把go图转变为二维矩阵，用0或1表示不同术语间的父子关系。对于每个样

本，label_embedding都是相同的。但面对内存问题（3万x3万的矩阵大小）

在解决内存问题的时候：

把label_embedding变成一维矩阵，把term当作单词/   用0或1表示有或无

基于本体的语义相似性：一种新的基于特征的方法 - ScienceDirect

总结：
1.

dense_matrix效果比sparse matrix更好，因为go术语等效于语言中的分词。稀

2.

3.

4.

疏矩阵过于稀疏，不利于特征提取。

对于label_embedding部分强烈建议使用Anc2vec算法

调大batch_size有助于loss更稳定，同时加快训练速度

消除batch effect不是通过输入一个个家族的，而是说在sequence_embedding

时，合并同源比对（PSSM，同源比对在CAFA5数据集中构建）的结果。

5.

训练的epoch最多1000

dense与sparse比较

-

前者更快！快多少倍？

