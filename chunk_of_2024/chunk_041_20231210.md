分区 文献阅读 的第 94 页

cl:
GRU通过递归机制对时间方面进行建模，而Transformer利用注意机制和位置编码145对长期依赖关系进行建模

cl:
这两张图显示了Feature Extraction的过程。使用编码器-解码器的最后一个隐藏层作为特征提取的结果。

cl:

cl:
统计数据是用了两次的，一次在图的特征提取中，也就是GRU和Transformer对应的过程中。另一次是在

Concatenation中。

天天没烦恼:

那gru直接三个全连接层是因为他使用的是递归机制，所以不需要平均池化嘛?

cl:

分区 文献阅读 的第 95 页

cl:

cl:
池化应该是在卷积后用的多，Transformer也认为是一种卷积。

cl:
GRU的话用全连接或许更好

cl:

作用都是压缩维度

cl:
皓哥说得对，GRU因为是递归，需要考虑序列前后的联系，如果直接池化，不利于捕获前后关系，也不利于拟合

天天没烦恼:

哦哦

天天没烦恼:

学到了

cl:

cl:
前面这篇，它在图表征结构时，用的是Node2vec，游走来捕获a和b的空间关联。但是a和b模型不知道它是氨基

酸，不知道a和b的生化性质。讨论里也说，如果把图表征里纳入氨基酸特征可能更好。其实也就是序列表征用两

次（和这个统计数据用两次一样）

分区 文献阅读 的第 96 页

多模态融合-面向生物医学数据

2023年12月10日

16:35