分区 基础知识 的第 164 页

炼丹。

分区 基础知识 的第 165 页

平行处理

分区 基础知识 的第 166 页

分区 基础知识 的第 167 页

分区 基础知识 的第 168 页

kernel_size=

依次

yici

分区 基础知识 的第 169 页

CAFA蛋白的GO，targets，会比one-hot。

tansformer，bert熟悉了之后，GNN的学习。开学前结束的。

上图的X是一个样本的数据，X是二维，对我们来说，X是一维。

里面有8个head，8个W是共享的吗。不能共享。

per_protein:(N, 1024)
per_residual:(N, len_amiod, 1024)

对于per_protein怎么用head，

分区 基础知识 的第 170 页

里面有8个head，8个W是共享的吗。不能共享。

per_residual:(N, len_amiod, 1024)

对于per_protein怎么用head，

第一步q与k相乘得到nkhd，第二步如何算》nhkd，

为什么它直接把embedding_size切成head个段，然后用不同的段与weight matrix相乘？这样是不是限制了注意力可关注的范围？

为什么在q_linear里，可以有input_dim -- input_dim, 可以有head_dim -- head_dim

len_q是什么意义？N是各个样本，len_q表示一个样本的序列长度，比如蛋白质氨基酸数据。

为什么传入的qkv都是input_data。不影响，他们通过linear后，变成了真正有区分的qkv

query_length 可以与key_value_length不同！self_attention是相同的，但是其它Attention可以不同

共同特征是，都把q变成（batch_size, q_len, heads, d_q)四维数据，(N * n_heads, query_sequence_pad_length, d_queries)

高维数据没法想象，
        # Before computing softmax weights, prevent queries from attending to certain keys

分区 基础知识 的第 171 页

自己的查询、键和值的线性变换？

分区 基础知识 的第 172 页

transformer
16:32
2023年8月3日

【太强啦！手把手从零实现Transformer算法教程！没见过比这更详细的教程】https://www.bilibili.com/video/BV1Mv411w7M9?vd_source=cbf56ca113eacf0cd09374095fd3d13f

详解自注意力机制中的位置编码（第一部分） - 知乎 (zhihu.com)

带注释的转换器 (harvard.edu)

•

•

resnet，或residual，解决梯度问题，防止梯度消失和特征退化。

layer normalization，区别于batch normalization

Bert就是transformer的encoder。

分区 基础知识 的第 173 页

训练：可以理解成encoder是在找到输入向量之间的相关性，而decoder是借助这种相关性进行预

测，再将预测结果和真实值做交叉熵作为训练的目标函数

分区 基础知识 的第 174 页

•

•

copy mechanism；你好我是库洛洛。库洛洛你好，(cid:1710)高兴认识你

guided Attention

•

beam search

•

损失函数优化

分区 基础知识 的第 175 页

•

加噪声

使用 BERT 和 TensorFlow 进行情感分析 |数据库大本营 (databasecamp.de)

分区 基础知识 的第 176 页

Diffusion model
2023年8月26日

9:52

反向去噪、Step

Denoise的内部

对于影像生成的扩散模型，文字信息如何添加

分区 基础知识 的第 177 页

Stable Diffusion

1是encoder，对文字、图片进行编码；

分区 基础知识 的第 178 页

Autoregressive和Diffusion两种生成latent representation的方式

FID、CLIP是对图片效果衡量的指标，

FID越小越好（真实结果组与生成结果组的representation的距离，FID-10K），

CLIP越大越好（Contrastive language-image pre-training，文字与图片匹配程度）

文字Encorder(cid:1710)重要，而Diffusion-model的大小对结果影响不大。

分区 基础知识 的第 179 页

generation model作用：吃text embedding，产生图片压缩的结果

decoder

Diffusion model在去噪时，噪声加在图片上；而generation model噪声加在中间产物上

generation model noise predicter 接受文字和图片的encode，预测噪声

去噪使得模糊的图，变为更为清晰

分区 基础知识 的第 180 页

GAN
2024年3月3日

8:56

Auto-encoder自编码模型

目标是输入输出尽可能接近
VAE

鉴别器

Using gradient descent to update the parameters in the generator,
but fix the discriminator

分区 基础知识 的第 181 页

Maximum likelihood estimation

G被训练为最小化伪造和真实样本之间的差异，而D被训练为最大化区分伪造和真是样本之间差异的置信度

分区 基础知识 的第 182 页

GANs的基本理论

生成对抗(cid:13697)络（generative adversarial networks，简称GAN）的主要思想来源于最小最大双人博弈[23]。基本 GAN 由一个
反映真实数据分布的生成器 G 和一个旨在区分真实数据和 G 生成的数据的鉴别器 D 组成（见图 2）。在训练过程中，GAN中
的两个模型会同时进行(cid:14362) 优化并相互竞争，以提高生成和判别(cid:14125)力，以找到纳什均衡，其中G被训练为最小化伪造和真实样
本之间的差异，(cid:13884)D被训练为最大化区分伪造和真实样本之间差异的(cid:13726)信度。因此，使用成本函数 V（G， D） 评估取决
于 G 和 D 的 minimax 双人游戏，即   最小值 麦克斯          ~  一个 一个[日志    ]   ~  [日志  −        ] 其中 x 是从实
数数据分布 p 中采样的数据(x），z 来(cid:14362)先验输入噪声变量 pz(z） 和  ⋅ )是期望。 们还定义了 pg(x）作为生成的数据分
布，G（z）作为G生成的数据，受分布p的约束数据和 D（x） 作为从 p 中采样 x 的概率数据.

1.
2.

下载：下载高分辨率图片 （ 35KB）
下载：下载全尺寸图像

图 2.基本 GAN 的结构。

在GAN训练过程中，一个模型是固定的，另一个模型是优化的[24]。首先，对发生器进行固定，判别器将真实样本尽可(cid:14125)地分
为正样本和负样本，以最大限度地提高判别精度，从(cid:13884)得到判别器的最优解为： 2  ⋆       一个 一个     一个 一个         .

分区 基础知识 的第 183 页

然后，对于固定 D，通过最小化 log（  − D（G（z））） 来训练生成器。(cid:13875)虑到学习早期的饱和情况， 们通常通过最大化
备择对数 D（G（z）） 来训练 G。理想情况下，当 pg = p数据，相当于      2.在实践中，鉴别器可(cid:14125)无法实现有限数据集的
理想优化。相反，优化过程在训练 D 的多次迭代和训练 G 的一次迭代之间交替进行。

来(cid:14362) <https://www.sciencedirect.com/science/article/pii/S0010482521003346#sec7>

然而，自Goodfellow在2014年提出GANs以来[24]，在训练中仍然存在一些具有挑战性的困难和不稳定的行为[25]。训练GAN的困难主要

来自于处理Jensen-Shannon（JS）散度和Kullback-Leibler（KL）散度，通常采用它们来测量发电机损耗并最小化发电机的损耗函数。

具体来说，由于生成的数据和真实数据几乎不可能有不可忽略的重叠，因此两个数据分布之间的JS散度近似于一个常数，导致梯度消

失。此外，由于KL散度的不同最优目标与损失函数JS散度之间的矛盾，梯度更新不稳定，对多样性和精度的惩罚不平衡导致模式崩溃，

即缺乏多样性。

JS divergence——JS散度
1.

如果两个分布没有重合，JS散度是log2，所以(cid:1710)难train original GAN

2.

所以出现了Least Square GAN（LSGAN）

Wasserstein GAN（WGAN）
Earth Mover's Distance 铲土，使两个分布接近。有(cid:1710)多plans，把它们穷举取最小

综上所述，与KL散度和JS散度相比，WGAN中使用的W距离具有优异的平滑性能，并解决了梯度消失问题。通过使用受限和参数化的判别

器神经网络，将W距离转换为可(cid:2124)解的形式，从而为生成器更新提供了有用的优化梯度。此外，近似的W距离可用于指示每次训练迭代中

的优化信息，作为训练进度的可靠指标，并且还与生成数据的质量相关。因此，与GAN相比，WGAN模型显著提高了训练稳定性，并有效

避免了模式崩溃。

来(cid:14362) <https://www.sciencedirect.com/science/article/pii/S0010482521003346#sec7>

1.

GAN只学习少样本, 生成的都归少样本

如果生成图像质量好，最直接的方式就是通过给GAN生成样本打伪标签（pseudo label）来

分区 基础知识 的第 184 页

2.

如果生成图像质量好，最直接的方式就是通过给GAN生成样本打伪标签（pseudo label）来

学习。但是伪标签怎么来呢？我们CVPR19的工作(https://github.com/NVlabs/DG-Net)就尝

试了最简单的方案，先用有label的data，训练一个teacher model，然后用teacher model

来给GAN生成的数据打基于概率的label，而有label的数据还是使用原始的label。 为啥要

额外用一个teacher model呢？是因为我们的样本是A和B生成一个AB的样本，所以单纯给A或

B的label都不太好。所以选择了teacher来预测。这样的好处就是label也比较smooth，不会

像one-hot的pseudo label那样，容易产生noise。

作者：郑哲东

链接：https://www.zhihu.com/question/388935271/answer/1193819675

来源：知乎

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

令人拍案叫绝的Wasserstein GAN - 知乎 (zhihu.com)

NeurIPS 2020 | 数据类别不平衡/长尾分布？不妨利用半监督或自监督学习 - 知乎 (zhihu.com)

【GAN】四、CGAN论文详解与代码详解 - 知乎 (zhihu.com)

条件生成对抗网络——cGAN原理与代码 - 知乎 (zhihu.com)

精巧的改造达到了惊艳的效果——WGAN+pytorch代码实现及代码详解 - 知乎 (zhihu.com)

latent space怎么用？一般的隐空间都是取噪声，这里用什么提示？用go-embedding？？

损失函数怎么看？收敛不收敛？minmaxV(G,D) = E data [logD(x)]+E z [log(1-D(G(z)))]

损失函数怎么(cid:2124)？torch.mean()

        loss_D = -torch.mean(discriminator(real_input_batch)) + torch.mean(discriminator(fake_input_batch))越接近-1

        loss_G = -torch.mean(discriminator(gen_input))负数，越接近0越好

D_loss的变化
在GAN的训练过程中，判别器的目标是最小化损失函数。

在进行了一系列的优化后，判别器可以识别出真实的数据和生成器生成的假数据，这个时候损失函数会趋近于0。这也意味着，生成器生成的假数据的质

量越高，判别器就越难判断出来，因此D_loss逐渐接近0。？？？

如果判别器无法准确地区分真实和假数据，也会导致D_loss增加，表示判别器分类错误的样本数量增加。这时候，生成器需要通过调整自己生成的假数据

的质量来提高模型的整体性能。

从公式中可以看出，在判别器的损失函数中，如果生成器生成的假数据被误认为是真实数据，则相应的概率值会趋近于1，此时D_loss会减小。如果生成

器生产的假数据被正确分类为假数据，则相应的概率值会趋近于0，此时D_loss会增加。

G_loss的变化
在GAN的训练过程中，生成器的目标是最大化D_loss。当判别器无法准确识别出生成器生成的假数据时，G_loss也会逐渐接近0。但是，生成器仅在被判别

器误认为是真实数据时才会获得奖(cid:1291)，因此，生成器需要不断提高自己的性能，使得其生成的数据更加逼真，进而提高真假分类的正确率。

从公式中可以看出，生成器的损失函数仅包含一项，即对于生成器生成的假数据，其被判别器分类为真实数据的概率。如果生成器生成的假数据与真实数

据非常像，则判别器(cid:1710)难区分它们，因此，G_loss会逐渐接近0。如果生成器的输出数据太过模糊或者偏离真实数据的分布，判别器就可以轻易地将其判

定为假数据，此时G_loss会增加。

链接：https://www.zhihu.com/question/595189325/answer/2981075569

分区 基础知识 的第 185 页

特征融合

2024年4月12日

23:12