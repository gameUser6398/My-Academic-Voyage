为了进一步辨别预训练对下游基准性(cid:14125)的影响， 们在不同的预训练持(cid:13597)时间后评估了
ProteinBERT。具体来说， 们沿着预训练从不同的快照启动模型，并在从这些状态微调后评估其下
游性(cid:14125)（图 3）。虽然有些任务没有从预训练中受益，但其他任务（例如二级结构和远程同源性）
从更多的预训练中显示出明显的收益，并且在这种改进中没有显示出饱和。鉴于这些是更具挑战性
的任务之一，这一点值得注意。

无花果。3.

分区 文献阅读 的第 74 页

预训练对下游任务的影响。在四个TAPE基准上微调的ProteinBE T模型的性(cid:14125)作为预训练量的函数
（通过加工蛋白质的数量来衡量）。所有九个基准的相似图显示在补充图S
 们还进行了消融测试，以研究GO注释预训练任务（补充图S2），并发现一些基准（特别是二级结
构、远程同源性和折叠类）从中受益。

3.3 蛋白质BERT在蛋白质长度上泛化
ProteinBERT的架构对不同的序列长度（即(cid:13638)码输入和输出序列的令牌数量）是高效和灵活的。为了
测试模型跨序列长度的泛化(cid:14125)力， 们在4个基准中的9个基准上测量了ProteinBERT的测试集性(cid:14125)，
这些基准在长度超过512个标记的蛋白质中具有不可忽略的测试集记录数量（图4）。具体来说，
们需要至少25个这样的记录，其中记录包含整个蛋白质（在全局任务的情况下）或残基（在局部任
务的情况下）。 们观察到，在大多数情况下，ProteinBERT对于较长的序列表现稍差，但只是适度
的，表明它确实在非常宽的蛋白质长度范围内泛化。此外，在某些情况下，较长的序列可以实现更
好的性(cid:14125)（例如，“主要PTM”基准中的16个384个令牌序列，或“神经(cid:14061)切割”基准中的1024个标
记序列）表明性(cid:14125)的变化可(cid:14125)是由于其他因素造成的（例如，预测较长序列的二级结构可(cid:14125)本质上
是一项更困难的任务）。

无花果。4.

在新标签页中打开下载幻灯片

跨序列长度的性(cid:14125)。具有不同输入序列长度的微调 ProteinBE T 模型的测试集性(cid:14125)。序列长度（例
如 5 2、 024 等）总是(cid:13638)码较短长度的蛋白质（例如，700 个残基的蛋白质将被(cid:13638)码为  024 长序
列）。箱线分布超过图 37  中使用的 3 个预训练快照

3.4 了解全球关注
为了证明全局注意力机制的内部工作原理， 们在微调该任务的模型之前和之后，提取了
ProteinBERT中从信号(cid:14061)基准测试集中选择的两种不相关蛋白质的24个注意力头的值（图5）。全球
注意力的模式在不同的蛋白质中明显不同，但存在一些共同的模式。例如，第3块中的注意力头#3倾
向于集中在蛋白质序列的开头，(cid:13884)同一层中的注意力头#2倾向于集中在其他部分。对信号(cid:14061)预测模
型进行微调似乎主要改变了最后一个（第6个）全局注意力层。例如，该层中的注意力头 #1 更改为
进一步强调序列的开头。在阳性示例中（图5，上图），注意力的最大增加是在信号(cid:14061)的末端（即切
割位点）。值得强调的是，确切的注意力值取决于从训练中获得的模型权重，这些权重可(cid:14125)会在运
行之间发生变化。根据 们的经验，微调往往会产生相当一致的结果，但有时会观察到微小的差
异。

无花果。5.

分区 文献阅读 的第 75 页

在新标签页中打开下载幻灯片

信号(cid:14061)预测微调前后的全球关注。两种选定蛋白质的全球关注值：淋病奈瑟菌中的外(cid:14284)蛋白
P.IIC（piiC）（顶部）和拟南芥中的γ碳酸酐酶样2线粒体蛋白（GAMMACAL2）（底部）。piiC在位
(cid:13726) -25处有一个信号(cid:14061)（以氨基酸序列SAA A结束）。GAMMACAL2没有信号(cid:14061)。左侧面板（红色）显
示了通用ProteinBE T模型获得的注意力值，经过预训练后，将其作为Uni ef90上的语言模型（但在
对任何特定任务进行微调之前）。热图显示了模型的 24 个注意力头中每个蛋白质残基处的全局注
意力值。条形图通过对所有注意力头的注意力值求和来显示每个残基处的总注意力。右图显示了在
信号(cid:14061)任务上微调ProteinBE T后注意力值的差异。热图显示所有位(cid:13726)和注意力头的注意力增加（(cid:13615)
色）或减少（紫色）。条形图通过对所有注意力头的差异求和来显示每个残基处注意力的总差异。
请注意，每个注意力头的总和必然达到  00%。因此，差异总和为 0%

4 讨论
 们介绍了ProteinBERT，这是一种用于蛋白质序列的新型深层语言模型，旨在以(cid:14362)然的方式捕获蛋
白质的局部和全局表示（图1）。 们已经证明了该模型的通用性，表明它可以在几分 内对各种蛋
白质任务进行微调，并获得接近最先进的结果（表2）。虽然一些较大的蛋白质语言模型[例如
ProtT5（Elnaggar等人，2021）]至少在某些测量任务上显示出更好的性(cid:14125)，但这些模型要大得多，
并且在预训练和推理期间涉及更多数量级的计算和内存。
与其他领先的蛋白质语言模型相比，ProteinBERT在大小，计算和内存方面非常节俭。例如，虽然
ProteinBERT 在单个 GPU 上进行了 4 周的预训练，但 UniRep 在 3 个 GPU 上训练了 5.4 周
（Alley 等人，2019 年），(cid:13884) ProtTrans 的 ProtT5-XL 是在具有数千个 GPU 和 TPU 的超级计算
机上训练的，并且太大(cid:13884)无法在大多数消费(cid:13877) GPU 上容纳单个序列（Elnaggar 等人， 2021）。
为了预训练ProteinBERT， 们引入了一种新的蛋白质注释预测预训练任务，该任务非常适合蛋白质

分区 文献阅读 的第 76 页

为了预训练ProteinBERT， 们引入了一种新的蛋白质注释预测预训练任务，该任务非常适合蛋白质
[与句子顺序预测和其他(cid:14362)然语言任务不同（Lan等人，2019）]。 们认为GO注释（Ashburner等
人，2000）是蛋白质语言建模的合理扩展。它们无处不在，可用于大部分精选蛋白质（UniRef46数
据集中约106M蛋白质的约90M）。此外，他们可以向模型传授广泛的蛋白质功(cid:14125)（从亚细(cid:14094)定位到途
径再到生化作用）。

与之前包含 ∼250M 假定冗余序列的工作不同（Rives 等人，2021 年）， 们将 ProteinBERT 的预

训练限制为 ∼106M 代表蛋白取(cid:14362) UniRef90（Suzek 等人，2007 年），在 UniProt 中 ∼215M 蛋
白的整个已知蛋白质空间中（Boutet 等人，2016 年). 们认为，使用一组非冗余的蛋白质更明
智，并且消除了由于蛋白质空间采样不均匀(cid:13884)导致的许多不必要的偏差，这在UniProt的非过滤版本
中很普遍。例如，UniProt 中有来(cid:14362)人类免疫(cid:13674)陷病毒 1 （HIV-1） 蛋白质组的 >1M 蛋白，尽管
真正的病毒仅包含 9 种蛋白质。这种冗余反映了HIV-1进化过程中序列变异的丰富性，以及研究人
员对这种变异的极大兴趣（与大多数其他研究较少的生物体相比）。使用一组非冗余的蛋白质也更
有效，特别是在预训练模型不到整个时代时（例如在寻找最佳超参数组合时）。
与传(cid:13583)的生物信息学工具不同，例如基于序列相似性的BLAST（Altschul et al.， 1990）和隐马尔
可夫模型（Finn et al.， 2014），它们基于序列相似性（因此需要通过大量数据库进行搜索），
这项工作中采用的深度学习方法仅使用主要序列信息，从(cid:13884)具有两个重要优势。首先，它允许大规
模快速推理和数据集构建。其次，这些模型可以在新序列存在的情况下保持有效，无论它们是否具
有同系物。
ProteinBERT的架构高效且高度可扩展，允许其处理任何长度的蛋白质序列。相同的模型权重符合任
何序列长度，允许在特定长度范围内对其进行训练，然后推广到其他看不见的序列长度（图 4）。
通过支持极长的序列（超过数万个残基），ProteinBERT 省去了将长序列拆分为较小块的复杂性，
这是基于(cid:14362) 注意的模型的常见做法，该模型随序列长度呈二次（(cid:13884)不是线性）增长
（Choromanski 等人，2020 年;扎希尔等人，2020 年）。该模型灵活性的核心是它使用全局注意力
层。全局注意力的紧凑性（相对于(cid:14362) 注意）也允许更容易地检查模型的注意力，因为所有注意力
值（在所有位(cid:13726)和注意力头上）都可以显示为简单的2D地图（图5），(cid:13884)不是需要覆盖所有(cid:14362) 注意
力的3D地图。
与语言建模领域的一般趋势兼容（Brown 等人，2020 年）， 们观察到 ProteinBERT 的较长时间
预训练显示出明显的性(cid:14125)提升，无论是作为语言模型（图 2）还是在许多特定任务中（图 3，补充
图S1).现有研究表明，在其他条件相同的情况下，更大的模型和额外的预训练与改进的模型性(cid:14125)相
关（Brown 等人，2020 年;德夫林等人，2018 年;里夫斯等人，2021 年）。因此， 们期望更大版
本的ProteinBERT（例如具有更多，更宽的层）会产生额外的改进。然(cid:13884)，即使使用这项工作中使用
的节俭计算资源（单个GPU），ProteinBERT也(cid:14125)与最先进的模型竞争（表2），为各种蛋白质任务提
供了简单高效的开箱即用解决方案。模型通过预训练学习的表示普遍适用于各种任务，使其可用于
涉及有限标记数据的少数镜头学习任务。
为了便于使用ProteinBERT， 们将预训练模型作为Python包[基于TensorFlow和Keras（Abadi等
人，2016;Chollet 等人，2015）]，它允许(cid:14362)动下载预训练模型状态，对标记的数据集进行微调和
评估，以及用于创建预训练数据集的(cid:14154)本。
通过提供有效且可访问的蛋白质序列和功(cid:14125)模型， 们希望加快蛋白质研究界对深度语言建模的采
用，并允许这一新的强大工具进一步推动蛋白质研究的界限。

分区 文献阅读 的第 77 页

Protein Structure Prediction: Challenges,

Advances, and the Shift of Research Paradigms

2023年7月15日

19:24