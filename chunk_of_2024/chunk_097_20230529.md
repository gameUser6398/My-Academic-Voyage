分区 个人心得 的第 252 页

第二周

2023年5月29日

20:57



懒狗的5.22-28：

目前在学习CNN，但是对于后面怎应用有点未知，包括转化蛋白质文本向量

batch——批次

epoch——时代，迭代次数



分区 个人心得 的第 253 页

赵丽洁

2023年5月22日

22:06



1.生物数据库

2.基因本体论（分子机制，生物过程，细胞位置）

3.蛋白质数据库

2023.5.29：

BERT（Bidirectional Encoder Representations from Transformers）是一种自然语言处理

（NLP）领域的预训练模型，它于2018年由Google开发并发布。BERT的出现标志着NLP领

域的重要突破，对于多项NLP任务都取得了领先的性能。

BERT模型的核心是基于Transformer架构的深度双向编码器。Transformer是一种基于自注

意力机制（self-attention）的模型架构，用于处理序列到序列（sequence-to-sequence）

的任务。通过自注意力机制，Transformer能够同时处理输入序列中的所有位置，从而捕捉全

局的语义关系。

BERT通过在大规模文本数据上进行预训练来学习通用的语言表示。这个预训练过程包括两个

主要任务：遮蔽语言模型（Masked Language Model，MLM）和下一句预测（Next

Sentence Prediction，NSP）。在遮蔽语言模型任务中，BERT模型会随机遮蔽输入文本中的

一些词(cid:2125)，然后预测被遮蔽的词(cid:2125)。而下一句预测任务则要(cid:2124)BERT判断两个句子是否是连续

的。

在预训练完成后，BERT可以通过微调（fine-tuning）的方式适应特定的下游任务，如文本分

类、命名实体识别、句子关系判断等。微调阶段，BERT模型的参数会在特定任务的训练数据

上进行进一步优化，使其能够更好地适应该任务。

BERT模型的优点在于它能够学习到更丰富的语言表示，尤其是通过双向编码器可以捕捉上下

文的语义信息。相比传统的基于循环神经网络（RNN）或卷积神经网络（CNN）的模型，

BERT在(cid:1710)多NLP任务上都取得了更好的性能。它的成功也为后续的模型如GPT-3等开创了道

路，推动了NLP领域的发展。

