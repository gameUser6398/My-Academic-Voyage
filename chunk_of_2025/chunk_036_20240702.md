分区 文献阅读 的第 55 页

们表明，在使用特定于任务的小型标记数据进行轻松微调后，单个预训练的转换器模型可以
同时在启动子、剪接位点和转录因子结合位点的预测方面实现最先进的性能。此外，
DNABERT 能够直接可视化输入序列中的核苷酸水平重要性和语义关系，从而更好地解释和准
确识别保守序列基序和功能遗传变异候选者。最后，我们证明，具有人类基因组的预训练
DNABERT甚至可以很容易地应用于其他具有卓越性能的生物体。我们预计预训练的DNABERT模
型可以针对许多其他序列分析任务进行微调。

DNABERT 使用标记化的 k-mer 序列作为输入，其中还包含一个 CLS 标记（代表整个句子含义

的标签）、一个 SEP 标记（句子分隔符）和 MASK 标记（用于表示预训练中的掩蔽 k-

mer）。输入通过嵌入层并被馈送到 12 个 Transformer 模块。最后一个隐藏状态中的第一个

输出将用于句子级分类，而单个掩码标记的输出将用于标记级分类

来自 <https://academic.oup.com/view-large/figure/446449769/btab083f1.tif>

scBERT 作为大规模预训练深度语言模型，用于单细胞
RNA-seq 数据的细胞类型注释

遵循 BERT 的预训练和微调方法，scBERT 通过对大量未标记的 scRNA-seq 数据进行预训练，

对基因-基因相互作用有了大致的了解;然后将其转移到看不见的和用户特异性的scRNA-seq数

据的细胞类型注释任务中，以进行监督微调。广泛而严格的基准研究验证了scBERT在细胞类型

注释、新型细胞类型发现、批次效应的稳健性和模型可解释性方面的卓越性能。

据我们所知，目前还没有关于将Transformer架构应用于基因表达数据分析的研究。最初设计

的端到端scBERT框架，具有基因表达嵌入和自学习策略，在细胞类型注释任务上具有优越的性

能、可解释性和泛化潜力。

来自 <https://www.nature.com/articles/s42256-022-00534-z>

使用大型语言模型预测抗微生物药物耐药性
在这项研究中，我们不仅使用了基于核苷酸序列的语言模型，还使用了基于 PubMed 文章的文

本语言模型，以反映模型中更多的生物学背景知识。我们提出了一种基于各种抗生素耐药基因

数据库的核苷酸序列语言模型和文本语言模型的微调方法。我们还提出了一种基于LLM的增强

技术来补充数据，并提出了一种集成方法来有效地结合两种模型。我们还提出了一个评估模型

的基准。我们的方法在耐药性类别预测方面取得了优于核苷酸序列语言模型的性能。

分区 文献阅读 的第 56 页

为了将基于预训练的核苷酸序列语言模型和前面提到的基于文本的预训练语言模型结合起来，

我们使用了软投票集成模型。

来自 <https://arxiv.org/html/2401.00642v1>

分区 文献阅读 的第 57 页

WGS结核病传播特征：中国全国横断面监测

2024年7月2日

15:53