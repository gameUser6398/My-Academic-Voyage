分区 刘畅乐 的第 272 页

•

•

base_channels 逐层变小：通常用于防止过拟合、减少计算开销，或在任务中特意聚焦更精细特征

时。

每层输出通道数不变：可以保持特征提取的一致性，简化模型结构，并在某些任务中提供平衡的

性能。

来自 <https://chat.eqing.tech/#/>

【深入思考】卷积网络（CNN）的平移不变性_cnn 平移不变-CSDN博客

8月31日

目前较优解：

base_channels：16

num_block：10

selfAttention一层或两层甚至0

防止过拟合，

lr要小0.00001，epoch至少3000

如何提高召回率：

1.

2.

3.

阈值设小，小于0.5

focalloss α设大

调整结构

9月1日
1.

  从ARGNet上，发现一层Attention就行，1d在压缩长度上很好

2.

对于v1_ResNet_0827，验证1层Attention和最佳nblock

超实用的7种 pytorch 网络可视化方法，进来收藏一波_torch模型可视化-CSDN博客

9月7日

1.

1_ResNet，通道设为32，层数设6

9月10日

深度学习中，是否应该打破正负样本1:1的迷信思想？ - 刘芷宁的回答 - 知乎
https://www.zhihu.com/question/654186093/answer/3537147315
在重采样之余，处理深度学习的不平衡有很多从其他方面入手的经典例子：

•

•

•

•

•

•

•

类别重加权：Class-Balanced Loss Based on Effective Number of Samples (CVPR
2019)

难例挖掘：Focal loss for dense object detection (ICCV 2017)

margin-based loss：Learning Imbalanced Datasets with Label-Distribution-
Aware Margin Loss (NIPS 2019)

meta-learning 自动学习加权/采样策略：

○

○

MESA: Boost Ensemble Imbalanced Learning with MEta -SAmpler (NeurIPS
2020)
Meta-weight-net: Learning an explicit mapping for sample weighting (NIPS
2019)

设计特殊模型架构：BBN: Bilateral-Branch Network with Cumulative Learning for
Long-Tailed Visual Recognition (CVPR 2020)

改变训练过程/策略：Decoupling Representation and Classifier for Long-tailed
Recognition (ICLR 2020)

后验概率校正：Posterior Re-calibration for Imbalanced Datasets (NeurIPS 2020)

9月11日

分区 刘畅乐 的第 273 页

9月12日

在配置好hydra后，发现了    optimizer.zero_grad()的错误，用上了schedule.step()

先后尝试了两个main，fc_hidden_sz略微变化，从45到41，41似乎更好

明天把per-residual数据整理下，filter_index是关键

输入通道8代表输入[8, 16,8]，最多顶

3层

输入通道16代表输入[16,8, 8]，最多

顶3层

输入通道4代表输入[4, 16, 16]，最多

顶4层

输入通道1代表输入[1, 32, 32]，最多

5层.

预期：保留12结果最好，

实际：相较于对比组均变差，保留

12结果最好

9月13日

在split_module任务中，删掉任意两块，

任务1 保留34   - input_channels: 8
      base_channels: 16
      kernel_size: 3
      stride: 1
      padding: 1
      downsample: True
      downsample_stride: 2
      fc_hidden_dim: 45
      num_classes: 41
      num_blocks: 5
      output_size: 1
    - input_channels: 16
      base_channels: 16
      kernel_size: 3
      stride: 1
      padding: 1
      downsample: True
      downsample_stride: 2
      fc_hidden_dim: 41
      num_classes: 41
      num_blocks: 5
      output_size: 1

2

3

保留14

保留12

调整34的层数，分别设为4、3层，效果变好

看来目前精准度是较差的，也就是把部分负的类预测成了正样本，灵敏度是够了

对比下Macro和Micro：
如果每个类别的样本数量差不多，那么宏平均和微平均没有太大差异
如果每个类别的样本数量差异很大，那么注重样本量多的类时使用微平均，注重样本量少的类
时使用宏平均
如果微平均大大低于宏平均，那么检查样本量多的类来确定指标表现差的原因
如果宏平均大大低于微平均，那么检查样本量少的类来确定指标表现差的原因

•
•

•
•

来自 <https://zhuanlan.zhihu.com/p/405658103>

对于perresidual

1.

2.

3.

4.

尝试best_input_channels

尝试best_block

尝试最佳kernel_size5

dropout 0.2/0.3

9月13日

评估指标很头疼，

已经尝试了几种计算f1score的方式：

1.

2.

3.

原始np.mean([batch_f1score, ….]

利用multilabelF1score.compute(）对每个batch收集的结果进行最终计算

事后计算，把所有的[N, C]个预测结果与target进行比较，计算f1score

分区 刘畅乐 的第 274 页

其中2和3相同，与1很不相同，差了0.2的f1score。

另有BinaryRecall和MultilabelRecall的区分，尽管都是用0和1来预测、

前者可达0.98，而后者最多0.91，

对于精准度也是如此

对于多分类任务，我将其视为多个二分类任务，也就是说模型会预测各个类别的概率，概率的

范围为0-1,。在这种情况下，使用BinaryRecall和MultilabelRecall对于统计整个epoch的召回率有

何影响？

那么，当我只用一个BinaryRecall，不区分类别来计算整体召回率，这不正是把所有样本所有类

别都拆分成样本，从而把多标签任务变成了二元多分类吗？这个理解有误吗

2024/9/14 17:01:02

9月15日
1.

对于gamma=2或3，focal_loss的alpha[0.5, 0.6, 0.75, 0.9]，alpha越大，召回率越

大，精准度越小

2.

在相同alpha下，gamma=2与3变化似乎不大。但是可能gamma越大，召回率和精准

度score会越差。

结论：alpha为0.75和gamma为2 是较优，无需研究！

9月16日
如何训练一个大模型：LoRA篇-阿里云开发者社区 (aliyun.com)

huggingface/peft： � PEFT： 最先进的参数高效微调。 (github.com)

facebookresearch/esm: Evolutionary Scale Modeling (esm): Pretrained language
models for proteins (github.com)

大模型参数高效微调技术实战（五）-LoRA - 知乎 (zhihu.com)

9月18日

有哪些算法惊艳到了你？ - 朱翔宇的回答 - 知乎
https://www.zhihu.com/question/26934313/answer/3115650451

↑：label smoothing， WGAN

分区 刘畅乐 的第 275 页

↑：label smoothing， WGAN

9约19日
esm2_loras/lora_esm2_script.py at main · Amelie-Schreiber/esm2_loras (github.com)

非transformer构架适应LoRA：Custom models (huggingface.co)

class ESMconfig 是否妥当

再读：Quantization (huggingface.co)

ysyisyourbrother/SYSU_Notebook: 本项目分享了中 大学计算机学院本科和研究生阶段的课

程资料、笔记、期末考试卷和其他实用的相关资源。希望对同学们的学习有所帮助�，如果

欢记得给个star� (github.com)

9月28日

目前，在进行了拆分数据集后，发现

的f1-score并不高，仅有0.7

所以接下来：

1.

2.

3.

DRSN到底好不好，需要对构架上进行调试

DRSN好在哪里，对噪声的处理，这符合ARG训练数据的特点

protein embedding的特点？噪声？

当前DRSN的结构是多尺度特征融合总结（金字塔结构）-CSDN博客

0923-165904

0928-164217 归一化

正则化dropout=0.3

正则化dropout=0.5

0928-232047

清理数据的类别

0928-235214

base_channels
都设为32

0928-235436

64

0928-235757 前2设为32，后二设为64

0928-235902 前2设为64，后二设为32

f1score=0.69

f1score=0.69左

差别不大，但是用上

右

了

0.81

0.82

0.83

0.815

0.825

效果显著提升！

有所提升

最好

比上一条好

0929-
kernel_size/

卷积核大小设为3

对focal loss进行尝试
--multirun focal_loss.alpha=
0.75,0.65,0.55
,0.85 focal_loss.gamma=1,2,3,4,5 &

9月29日
1.

能否尝试对base_channels逐层增加？

2.

能否替换掉stride=2

1.

DRSN的有点

预测因子和结果的定义和测量不一致或缺乏盲法

来自 <https://www.bmj.com/content/386/bmj-2023-078276.long>

11月30日

使用per-protein嵌入，利用DRSN二维卷积

尽可能保持位置特征。

学习率1e-4

判断四个并行的通道是否学到不同的特征（不用管

判断attention分数是否有效（查酶功能设计相关）

判断

12月7日

分区 刘畅乐 的第 276 页

12月7日

计划：

•

•

•

conv1d最多接受3维输入，所以drsn1d的input_channels和output_channels设为

max_length

改drsn1d的头部cnn为conv2d，后面仍为conv1d

attention hiddensize 从1变为40？

v1, drsn1d

12月7日22时，conv1d（512,512），

zesty-plant-32 0.027768253612188588

12月10日，linear改为cosine，lr2e-4

11日发现model的策略需要重构，尤其是attention

v2, drsn2d

12月11日，lr1e-4 不再尝试

有个漏洞，这两个版本虽然保留了长度不变，但未必能体现在氨

基酸上

如果将最后的尺寸进行per-protein的池化？

12月14日

对于drsn1d，尝试了新的方向上的卷积。效果是否提升？
答：比较结果在lora_esm_drsn_v4中，最新尝试vs前期尝试

结果上并没有显著差异。有待评估

12月15日

MultiTaskTrainer设置完成。

在dataloader时无法分离dc和mc

在计算loss时，alpha * L2 + beta * L2

在评估时新设定了metrics函数

•

•

•

如何

12月16日

这对验证集loss反弹，但F1等指标不变。这与F1的阈值有关，反映了模型的学习从确定到不

确定。

决定从以下方面调整：

降低学习率，2e-4,  1e-4,  1.5e-4。

差别不大？不是问题

•

降低模型复杂度同时增大kernel size。3层DRSN 不是问题

放弃self attention，直接classifier

不是问题

问题在于base_model需要低学习率，但是CNN需要高学习率

12月16日晚

对模型构架进行调整：

1、kernel size =3，7; n_block=3; channel 480->60

2、不同para 特征相加，维持位置不变形

3、直接linear classifier

此外，对base_model和DRSN的学习率进行分组优化：

dulcet-flower-8 不明显

(2e-5，5e-5)

（2e-5, 1e-4)提高CNN学习率试试

9

(2e-5, 2e-5)

12月17日

使用focal loss

学习率

日志 效果

（2e-4, 5e-5) 2

(2e-5, 1e-4)

1

与BCE相似，仍出现反弹

使用warm-up

学习率（2e-4, 5e-5) misty-glitter-8 好转，不到哪里去。

分区 刘畅乐 的第 277 页

(5e-6, 1e-5)

9

来自 <https://wandb.ai/3494013734-east-china-normal-university/Multitask-m2-l2/runs/yeqxi310>

12月17日晚（周二）

超参数搜索，寻找DRSN最优学习率。目标是尽可能低eval_loss，高F1，以dc为主

当前训练策略：warmup_ratio=0.1; focal loss, 0.75, 2; epoch=

base_model_l

r: (5e-6, ?

"learning_rate":
{"distribution": "uniform",
"min": 1e-6, "max": 5e-5}

drsn_lr | Multitask-m2-l2
Workspace – Weights & Biases

（1e-5, ?

改变

base_model_l
r：

默认drsn学习

率：

(?, 5e-5)

(

(4e-5, ?

"learning_rate":
{"distribution": "uniform",
"min": 1e-6, "max": 5e-5}

3fw2w282 | Multitask-m2-l2
Workspace – Weights & Biases

"learning_rate":
{"distribution": "uniform",
"min": 4e-5, "max": 1.5e-4}

https://wandb.ai/3494013734-east-
china-normal-university/Multitask-
m2-l2/sweeps/3ze7nned

drsn_lr:

越大越

好。

预期，越

大越好

base_lr:4.5
e-5。

越大越好

(4e-5, ?

重复,epoch 50,改变warmup Weights & Biases

2.

3.

4.

为什么不把lora放在最后？

lora的attention为什么不变,时因为r太小了吗

利用attention mask信息,在attention之外的部分可以在计算时deactivate

warmup epoch

Trick : PyTorch学习率 warm up + 余弦退火_warm up 余弦退火更新学习率-CSDN博客

2025年2月4日

分区 刘畅乐 的第 278 页

效果比对日志
13:21

2024年11月15日

ARGnet

只识别出5个ARG？？有以下几个可能：

•

•

•

fasta读取？

自回归模型效果差。

环境包的问题

在预测是否是ARG时采用自回归模型，如果重建序列与原始序列达到一定程度相似性，则认为该序列为ARG

分区 刘畅乐 的第 279 页

在预测是否是ARG时采用自回归模型，如果重建序列与原始序列达到一定程度相似性，则认为该序列为ARG

Resfinder（舍弃）
resfinder · PyPI
cadms/resfinder

2月10日

目标：数据五折拆分。不同工具五折验证。

2月14日

注意test_fold需要在打开计算指标时读取，否则提前读取导致某些序列因未被预测到而失去

test_fold

2月15日

要展示模型的性能，需要比较。

1.

以PLM为准的性能，热图。但

分区 刘畅乐 的第 280 页

1.
2.

3.

multi-drug，β-lactam子类图，

分区 刘畅乐 的第 281 页

效果提升策略
11:00

2024年12月27日

1.

训练数据集是全集，怎么划分序列一致性？

BLAST

抽取30%序列，与其它70%序列作匹配，查看他们的标签一致性。循环100次

CD-hit

查看不同阈值下，标签一致性？过于粗糙。

○

○

○

○

2.

阿是

分区 刘畅乐 的第 282 页

ESM微调日志

2024年9月23日

0:04