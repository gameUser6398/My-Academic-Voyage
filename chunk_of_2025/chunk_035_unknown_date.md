

基因组学大型模型 |自然方法 (nature.com)

1.

2.

3.

转录组？而非基因组？

DNABERT可用于基因嵌入，然后呢？怎样识别SNP对功能的影响？

基因组大语言模型不如蛋白质大语言模型火啊。

迁移学习支持网络生物学的预测

绘制基因网络图谱需要大量的转录组数据来学习基因之间的联系，这阻碍了
在数据有限的环境中的发现，包括罕见疾病和影响临床上无法接近的组织的
疾病。最近，迁移学习彻底改变了自然语言理解等领域1,2和计算机视觉3通
过利用在大规模通用数据集上预训练的深度学习模型，然后可以针对具有有
限任务特定数据的大量下游任务进行微调。在这里，我们开发了一种基于上
下文感知、基于注意力的深度学习模型 Geneformer，该模型在大约 3000
万个单细胞转录组的大规模语料库上进行了预训练，以便在网络生物学数据
有限的环境中实现特定于上下文的预测。在预训练期间，Geneformer对网络
动力学有了基本的了解，以完全自我监督的方式在模型的注意力权重中对网
络层次结构进行了编码。使用有限的特定任务数据对与染色质和网络动力学
相关的各种下游任务进行微调，表明 Geneformer 始终如一地提高了预测准
确性。应用于患者数据有限的疾病建模，Geneformer确定了心肌病的候选治
疗靶点。总体而言，Geneformer代表了一种预训练的深度学习模型，从中可
以针对广泛的下游应用进行微调，以加速关键网络调节剂和候选治疗靶点的
发现。

DNABERT：来自基因组中 DNA 语言的 Transformer 模型
的预训练双向编码器表示（似乎与功能无关，更关注调
控密码）

破译非编码DNA的语言是基因组研究的基本问题之一。由于多义性和远距离语义关系的存
在，基因调控密码非常复杂，以前的信息学方法往往无法捕捉到这一点，尤其是在数据稀缺
的情况下。
为了应对这一挑战，我们开发了一种名为 DNABERT 的新型预训练双向编码器表示，以捕获
基于上游和下游核苷酸上下文的基因组 DNA 序列的全局和可转移理解。我们将DNABERT与最
广泛使用的全基因组调控元件预测程序进行了比较，并证明了其易用性、准确性和效率。我
们表明，在使用特定于任务的小型标记数据进行轻松微调后，单个预训练的转换器模型可以

