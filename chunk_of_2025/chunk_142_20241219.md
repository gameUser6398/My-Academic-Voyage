分区 刘畅乐 的第 283 页

9月24日

要汇总问题

如何像专业人士一样在团队中使用 Git 和 GitHub – 以 Harry 和 Hermione � 为特色
(freecodecamp.org)

11月30日

默认参数LORA微调ESM，利用transformer直接分类

步骤：

1.

2.

学习率调整，约5e-5

秩和缩放因子，

（8，32），12月3日，9点结束

（4，16），12月4日，21点开始

（16，64），12月5日，15点开始

（8，16），12月6日，22点开始，效果最好

3.

调整target_modules为
['query', 'value', 'key']  # key is new added

保存参数有问题，只能加载ckpt

针对ESM2模型规模为35M，数据集约30000条序列的情况，确定学习率可以遵循以下建议：

1.

较小的学习率：由于您的数据集相对较小（30000条序列），建议使用较小的学习率。这样可以避

免在微调过程中对预训练模型的参数进行过大的更新，从而保留预训练模型已经学习到的知识。

2.

预训练学习率的百分比：您可以将微调学习率设置为预训练阶段最大学习率的10%。如果预训练阶

段的学习率是3e-4，那么微调学习率可以选择为3e-5。

3.

Batch size的调整：记得根据您的batch size对学习率进行相应的缩放。通常，学习率的缩放倍

数为batch size倍数的开方。例如，如果batch size增大了4倍，学习率可以扩大2倍。

4.

Warmup策略：使用warmup策略，在训练初期逐渐增加学习率，然后再按一定策略逐渐降低，这有

助于模型在训练初期稳定收敛。

来自 <https://kimi.moonshot.cn/chat/ctfs09nfe5ugb236pugg>

分区 刘畅乐 的第 284 页

特征提取日志
19:42

2024年12月19日

关注的内容：

1.

2.

Attention识别结构域

经典已有结构域的识别

主要方法：

1.

bertviz model_view可视化工具，

○

○

帮助识别ESM的12层的识别模式变化。最后几层可能才关注到句子关系和特定的

token上

多头attention识别模式解析。不同头识别不同模式，发现某些相关的模式。根据

(转)语言模型如何将注意力权重放在长上下文上？ - 知乎，其中不同头的模式可

以有6类。不适用，不属于长上下文。

2.

3.

熵识别保守位置？

简单平均或取最大值

分区 刘畅乐 的第 285 页

不平衡策略评估

2024年10月9日

14:16